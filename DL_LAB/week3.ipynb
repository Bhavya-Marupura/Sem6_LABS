{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072f212a-3a03-42d3-90df-2deec0507c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a32a145-510d-43c7-9c37-ea7157603444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23]) torch.Size([23])\n",
      "The parameters are tensor([0.4820], requires_grad=True), and tensor([0.1916], requires_grad=True)\n",
      "The parameters are w= tensor([0.7198], requires_grad=True), b= tensor([0.2050], requires_grad=True), and loss= 45.53408432006836\n",
      "The parameters are w= tensor([0.8094], requires_grad=True), b= tensor([0.2101], requires_grad=True), and loss= 6.476458549499512\n",
      "The parameters are w= tensor([0.8432], requires_grad=True), b= tensor([0.2120], requires_grad=True), and loss= 0.9305087327957153\n",
      "The parameters are w= tensor([0.8559], requires_grad=True), b= tensor([0.2127], requires_grad=True), and loss= 0.14301641285419464\n",
      "The parameters are w= tensor([0.8607], requires_grad=True), b= tensor([0.2130], requires_grad=True), and loss= 0.031197434291243553\n",
      "The parameters are w= tensor([0.8625], requires_grad=True), b= tensor([0.2131], requires_grad=True), and loss= 0.015319795347750187\n",
      "The parameters are w= tensor([0.8632], requires_grad=True), b= tensor([0.2131], requires_grad=True), and loss= 0.013065120205283165\n",
      "The parameters are w= tensor([0.8634], requires_grad=True), b= tensor([0.2131], requires_grad=True), and loss= 0.012744957581162453\n",
      "The parameters are w= tensor([0.8635], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012699441984295845\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012692919000983238\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691942043602467\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691771611571312\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691634707152843\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691594660282135\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691507115960121\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691454961895943\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691405601799488\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691369280219078\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691271491348743\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2132], requires_grad=True), and loss= 0.012691193260252476\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012691152282059193\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012691089883446693\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012691037729382515\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690970674157143\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690898962318897\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690848670899868\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690803036093712\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690703384578228\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690645642578602\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.01269060093909502\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690553441643715\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2133], requires_grad=True), and loss= 0.012690485455095768\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690401636064053\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690344825387001\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690272182226181\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690243311226368\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690153904259205\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.01269006822258234\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012690027244389057\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012689981609582901\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012689875438809395\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012689823284745216\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.01268978975713253\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.012689727358520031\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2134], requires_grad=True), and loss= 0.01268964633345604\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689587660133839\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689531780779362\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689454481005669\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.01268940232694149\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689332477748394\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689280323684216\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689230032265186\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689166702330112\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689057737588882\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012689025141298771\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012688937596976757\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012688925489783287\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2135], requires_grad=True), and loss= 0.012688819319009781\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688758783042431\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688693590462208\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688667513430119\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.01268855482339859\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688495218753815\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.01268847193568945\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688412331044674\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.01268836110830307\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688254937529564\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688199058175087\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.012688136659562588\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2136], requires_grad=True), and loss= 0.01268809475004673\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012688023038208485\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687945738434792\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687882408499718\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.01268785260617733\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.01268775574862957\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687692418694496\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687653303146362\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.0126876225695014\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687518261373043\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687445618212223\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687391601502895\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687332928180695\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2137], requires_grad=True), and loss= 0.012687278911471367\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012687183916568756\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012687141075730324\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012687087059020996\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012687024660408497\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686945497989655\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686884962022305\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686829082667828\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686756439507008\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686733156442642\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686623260378838\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686568312346935\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686541303992271\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2138], requires_grad=True), and loss= 0.012686485424637794\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2139], requires_grad=True), and loss= 0.012686368077993393\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2139], requires_grad=True), and loss= 0.012686335481703281\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2139], requires_grad=True), and loss= 0.012686283327639103\n",
      "The parameters are w= tensor([0.8636], requires_grad=True), b= tensor([0.2139], requires_grad=True), and loss= 0.012686233036220074\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Q1\n",
    "x=torch.tensor([12.4,14.3,14.5,14.9,16.1,16.9,16.5,15.4,17.0,17.9,18.8,20.3,22.4,19.4,15.5,16.7,17.3,18.4,19.2,17.4,19.5,19.7,21.2])\n",
    "y=torch.tensor([11.2,12.5,12.7,13.1,14.1,14.8,14.4,13.4,14.9,15.6,16.4,17.7,19.6,16.9,14.0,14.6,15.1,16.1,16.8,15.2,17.0,17.2,18.6])\n",
    "print(x.shape,y.shape)\n",
    "b=torch.rand([1],requires_grad=True)\n",
    "w=torch.rand([1],requires_grad=True)\n",
    "print(f\"The parameters are {w}, and {b}\")\n",
    "learning_rate=torch.tensor(0.001)\n",
    "loss_list=[]\n",
    "for iter in range(100):\n",
    "    loss=0.0\n",
    "    for j in range(len(x)):\n",
    "        a=w*x[j]\n",
    "        y_pred=a+b\n",
    "        loss+=(y_pred-y[j])**2\n",
    "    loss=loss/len(x)\n",
    "    loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.grad\n",
    "        b-=learning_rate*b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    print(f\"The parameters are w= {w}, b= {b}, and loss= {loss.item()}\")\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b07687-f8ea-46dc-9bd8-39db98ba44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([2])\n",
      "The parameters are 1.0, and 1.0\n",
      "w.grad = -174.0, b.grad = -52.0\n",
      "Epoch= 1 : The parameters are w= 1.1740000247955322, b= 1.0520000457763672, and loss= 757.0\n",
      "w.grad = -170.20799255371094, b.grad = -50.85199737548828\n",
      "Epoch= 2 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 724.3797607421875\n",
      "Analytical Solution\n",
      "w = 0.79,b = 0.9299999999999999,loss = 26.0\n",
      "w = 0.57454,b = 0.8581799999999999,loss = 39.7\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "x=torch.tensor([2.0,4.0])\n",
    "y=torch.tensor([20.0,40.0])\n",
    "print(x.shape,y.shape)\n",
    "b=torch.tensor(1.0,requires_grad=True)\n",
    "w=torch.tensor(1.0,requires_grad=True)\n",
    "print(f\"The parameters are {w}, and {b}\")\n",
    "learning_rate=torch.tensor(0.001)\n",
    "loss_list=[]\n",
    "for iter in range(2):\n",
    "    loss=0.0\n",
    "    for j in range(len(x)):\n",
    "        a=w*x[j]\n",
    "        y_pred=a+b\n",
    "        loss+=(y_pred-y[j])**2\n",
    "    loss=loss/len(x)\n",
    "    loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    print(f\"w.grad = {w.grad}, b.grad = {b.grad}\")\n",
    "    with torch.no_grad():\n",
    "        w-=learning_rate*w.grad\n",
    "        b-=learning_rate*b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    print(f\"Epoch= {iter+1} : The parameters are w= {w}, b= {b}, and loss= {loss.item()}\")\n",
    "\n",
    "inp_x = np.array([2,4])\n",
    "inp_y = np.array([20,40])\n",
    "def analytical(x,y,w,b):  \n",
    "    loss = 0.0\n",
    "    for epochs in range(2):\n",
    "        for j in range(len(x)):\n",
    "            y_p = w*x[j] + b\n",
    "            loss += (y[j] - y_p)\n",
    "        loss = loss/len(x)\n",
    "        wgrad,bgrad = 0,0\n",
    "        for i in range(len(x)):\n",
    "            wgrad += (y[j]-y_p)*(x[i])\n",
    "            bgrad += (y[j]-y_p)\n",
    "        w -= 0.001*wgrad*2/len(x)\n",
    "        b -= 0.001*bgrad*2/len(x)\n",
    "        print(\"w = {},b = {},loss = {}\".format(w,b,loss))\n",
    "\n",
    "print(\"Analytical Solution\")\n",
    "analytical(inp_x,inp_y,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f374d20-befd-4d62-8cfc-f4aa03c8b3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch= 1 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 26548.552734375\n",
      "Epoch= 2 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 11453.1484375\n",
      "Epoch= 3 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 5099.8251953125\n",
      "Epoch= 4 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 2425.84716796875\n",
      "Epoch= 5 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 1300.423583984375\n",
      "Epoch= 6 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 826.75146484375\n",
      "Epoch= 7 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 627.3866577148438\n",
      "Epoch= 8 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 543.4717407226562\n",
      "Epoch= 9 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 508.1470642089844\n",
      "Epoch= 10 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 493.2731018066406\n",
      "Epoch= 11 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 487.00616455078125\n",
      "Epoch= 12 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 484.36175537109375\n",
      "Epoch= 13 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 483.24212646484375\n",
      "Epoch= 14 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.76422119140625\n",
      "Epoch= 15 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.556396484375\n",
      "Epoch= 16 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.46221923828125\n",
      "Epoch= 17 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.4159240722656\n",
      "Epoch= 18 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.3893127441406\n",
      "Epoch= 19 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.37176513671875\n",
      "Epoch= 20 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.35736083984375\n",
      "Epoch= 21 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.3450622558594\n",
      "Epoch= 22 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.33282470703125\n",
      "Epoch= 23 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.3211975097656\n",
      "Epoch= 24 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.3094177246094\n",
      "Epoch= 25 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.2977600097656\n",
      "Epoch= 26 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.2862243652344\n",
      "Epoch= 27 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.27471923828125\n",
      "Epoch= 28 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.263427734375\n",
      "Epoch= 29 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.25189208984375\n",
      "Epoch= 30 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.240234375\n",
      "Epoch= 31 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.22869873046875\n",
      "Epoch= 32 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.2176208496094\n",
      "Epoch= 33 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.20562744140625\n",
      "Epoch= 34 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1944885253906\n",
      "Epoch= 35 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.18316650390625\n",
      "Epoch= 36 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1715393066406\n",
      "Epoch= 37 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1602478027344\n",
      "Epoch= 38 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1485900878906\n",
      "Epoch= 39 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1372985839844\n",
      "Epoch= 40 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1256408691406\n",
      "Epoch= 41 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.11456298828125\n",
      "Epoch= 42 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.1031188964844\n",
      "Epoch= 43 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.0914611816406\n",
      "Epoch= 44 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.0804748535156\n",
      "Epoch= 45 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.069091796875\n",
      "Epoch= 46 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.0577087402344\n",
      "Epoch= 47 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.04638671875\n",
      "Epoch= 48 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.0347595214844\n",
      "Epoch= 49 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.02349853515625\n",
      "Epoch= 50 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.01220703125\n",
      "Epoch= 51 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 482.00091552734375\n",
      "Epoch= 52 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.98944091796875\n",
      "Epoch= 53 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.9781799316406\n",
      "Epoch= 54 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.96710205078125\n",
      "Epoch= 55 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.95556640625\n",
      "Epoch= 56 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.94451904296875\n",
      "Epoch= 57 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.9332580566406\n",
      "Epoch= 58 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.9219665527344\n",
      "Epoch= 59 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.91058349609375\n",
      "Epoch= 60 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.89923095703125\n",
      "Epoch= 61 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.8880920410156\n",
      "Epoch= 62 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.87677001953125\n",
      "Epoch= 63 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.86590576171875\n",
      "Epoch= 64 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.8544921875\n",
      "Epoch= 65 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.8433532714844\n",
      "Epoch= 66 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.832275390625\n",
      "Epoch= 67 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.8209533691406\n",
      "Epoch= 68 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.80999755859375\n",
      "Epoch= 69 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.7984924316406\n",
      "Epoch= 70 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.7872009277344\n",
      "Epoch= 71 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.7762756347656\n",
      "Epoch= 72 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.76513671875\n",
      "Epoch= 73 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.75384521484375\n",
      "Epoch= 74 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.74273681640625\n",
      "Epoch= 75 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.7315979003906\n",
      "Epoch= 76 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.720458984375\n",
      "Epoch= 77 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.7093200683594\n",
      "Epoch= 78 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.6981506347656\n",
      "Epoch= 79 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.68731689453125\n",
      "Epoch= 80 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.67596435546875\n",
      "Epoch= 81 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.6649475097656\n",
      "Epoch= 82 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.65399169921875\n",
      "Epoch= 83 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.64276123046875\n",
      "Epoch= 84 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.631591796875\n",
      "Epoch= 85 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.6207580566406\n",
      "Epoch= 86 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.60955810546875\n",
      "Epoch= 87 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.59869384765625\n",
      "Epoch= 88 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.587890625\n",
      "Epoch= 89 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.57635498046875\n",
      "Epoch= 90 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.5655822753906\n",
      "Epoch= 91 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.55438232421875\n",
      "Epoch= 92 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.5438537597656\n",
      "Epoch= 93 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.53253173828125\n",
      "Epoch= 94 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.5213928222656\n",
      "Epoch= 95 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.5105895996094\n",
      "Epoch= 96 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.49945068359375\n",
      "Epoch= 97 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.48834228515625\n",
      "Epoch= 98 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.47802734375\n",
      "Epoch= 99 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.4668884277344\n",
      "Epoch= 100 : The parameters are w= 1.344208002090454, b= 1.1028521060943604, and loss= 481.45562744140625\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwn0lEQVR4nO3deXRUZZ7/8U9lqUqQVMJiEiJhb2URQVli2qXbIT8CZpxGPTMNMg6NiIMdeoT0IDKtyLTjhMHRdkP5OTOKfcYNzs+lBYXOhK1pwmI0yiJpF+jQQgVlqQoQstXz+wPrQg3QJpB7b1J5v865R1L3ya2nHg/U59z7fZ7HY4wxAgAAiDFxbncAAADADoQcAAAQkwg5AAAgJhFyAABATCLkAACAmETIAQAAMYmQAwAAYhIhBwAAxKQEtzvgpnA4rP379yslJUUej8ft7gAAgGYwxqimpkZZWVmKizv//ZoOHXL279+v7Oxst7sBAAAuwL59+9SzZ8/znu/QISclJUXSqUHy+/0u9wYAADRHKBRSdna29T1+Ph065EQeUfn9fkIOAADtzHeVmlB4DAAAYhIhBwAAxCRCDgAAiEmEHAAAEJMIOQAAICYRcgAAQEwi5AAAgJhEyAEAADGJkAMAAGISIQcAAMQkQg4AAIhJhBwAABCTOvQGnXZ54reVCtY2qPCmAUr3J7ndHQAAOiTu5Njg9W379HLZH/XNsXq3uwIAQIdFyLFBYvypYW1oCrvcEwAAOi5Cjg28CYQcAADcRsixQWK8R5JUT8gBAMA1hBwbnH5cZVzuCQAAHRchxwZWyGnkTg4AAG4h5NjAS+ExAACuI+TYIDGBmhwAANxGyLEBNTkAALiPkGMD1skBAMB9hBwbUJMDAID7CDk2sNbJYXYVAACuIeTYgJocAADcR8ixQSLbOgAA4DpCjg2oyQEAwH2EHBuwdxUAAO5rUcgpLi7WqFGjlJKSovT0dE2YMEGVlZVRbX74wx/K4/FEHTNmzIhqU1VVpYKCAnXq1Enp6emaM2eOGhsbo9qsW7dO11xzjXw+nwYMGKClS5ee1Z/FixerT58+SkpKUk5OjrZu3dqSj2Ob09s6UJMDAIBbWhRy1q9fr8LCQm3evFklJSVqaGjQ2LFjdfz48ah206dP14EDB6xj0aJF1rmmpiYVFBSovr5emzZt0ssvv6ylS5dq/vz5Vps9e/aooKBAN910kyoqKjRr1izdfffdWr16tdXmjTfeUFFRkR5++GF9+OGHGjZsmPLz83Xw4MELHYtWwzo5AAC4z2OMueDbDV9//bXS09O1fv163XjjjZJO3ckZPny4nnzyyXP+zvvvv6+//Mu/1P79+5WRkSFJWrJkiebOnauvv/5aXq9Xc+fO1cqVK7Vjxw7r9yZOnKijR49q1apVkqScnByNGjVKzz77rCQpHA4rOztbP/vZz/TAAw80q/+hUEipqakKBoPy+/0XOgxnWbz2cz22ulITR2Vr4e1Xtdp1AQBA87+/L6omJxgMSpK6du0a9forr7yi7t2768orr9S8efN04sQJ61xZWZmGDh1qBRxJys/PVygU0s6dO602eXl5UdfMz89XWVmZJKm+vl7l5eVRbeLi4pSXl2e1OZe6ujqFQqGoww7U5AAA4L6EC/3FcDisWbNm6brrrtOVV15pvX7HHXeod+/eysrK0ieffKK5c+eqsrJSb775piQpEAhEBRxJ1s+BQODPtgmFQqqtrdWRI0fU1NR0zja7d+8+b5+Li4v1z//8zxf6kZuNdXIAAHDfBYecwsJC7dixQxs3box6/Z577rH+PHToUPXo0UNjxozRF198of79+194T1vBvHnzVFRUZP0cCoWUnZ3d6u9zuvCYOzkAALjlgkLOzJkztWLFCm3YsEE9e/b8s21zcnIkSZ9//rn69++vzMzMs2ZBVVdXS5IyMzOt/0ZeO7ON3+9XcnKy4uPjFR8ff842kWuci8/nk8/na96HvAiskwMAgPtaVJNjjNHMmTP11ltvac2aNerbt+93/k5FRYUkqUePHpKk3Nxcbd++PWoWVElJifx+vwYPHmy1KS0tjbpOSUmJcnNzJUler1cjRoyIahMOh1VaWmq1cVNiAjU5AAC4rUV3cgoLC/Xqq6/qnXfeUUpKilVDk5qaquTkZH3xxRd69dVXdfPNN6tbt2765JNPNHv2bN1444266qpTs4zGjh2rwYMH684779SiRYsUCAT04IMPqrCw0LrLMmPGDD377LO6//77ddddd2nNmjVatmyZVq5cafWlqKhIU6ZM0ciRIzV69Gg9+eSTOn78uKZOndpaY3PBEuK4kwMAgOtMC0g65/HSSy8ZY4ypqqoyN954o+natavx+XxmwIABZs6cOSYYDEZdZ+/evWb8+PEmOTnZdO/e3fz85z83DQ0NUW3Wrl1rhg8fbrxer+nXr5/1Hmd65plnTK9evYzX6zWjR482mzdvbsnHMcFg0Eg6q38X6/3tB0zvuSvMbc/9vlWvCwAAmv/9fVHr5LR3dq2Ts2Z3te5a+oGu6pmq38y8vtWuCwAAHFonB+cWmV1Vz+wqAABcQ8ixAds6AADgPkKODVgMEAAA9xFybMA6OQAAuI+QY4PIOjmEHAAA3EPIsQGFxwAAuI+QYwMvNTkAALiOkGMDZlcBAOA+Qo4NEuNP1eQ0ho3CYe7mAADgBkKODRITTg9rQ5i7OQAAuIGQY4NITY5EXQ4AAG4h5Ngg8cyQwwwrAABcQcixQXycR3GnynIoPgYAwCWEHJtYa+UQcgAAcAUhxyaslQMAgLsIOTaJzLDicRUAAO4g5NgkslYOWzsAAOAOQo5NWPUYAAB3EXJsQk0OAADuIuTYhDs5AAC4i5Bjk8SEb2tyCDkAALiCkGMT604OhccAALiCkGOTRGpyAABwFSHHJl5qcgAAcBUhxybWOjmEHAAAXEHIsQmzqwAAcBchxybWtg4UHgMA4ApCjk1YDBAAAHcRcmxCTQ4AAO4i5NiEmhwAANxFyLEJIQcAAHcRcmziTaAmBwAANxFybGLV5DC7CgAAVxBybMLjKgAA3EXIsQkhBwAAdxFybMI6OQAAuIuQYxPWyQEAwF2EHJuwrQMAAO4i5NgkUpPTGOZxFQAAbiDk2MRL4TEAAK4i5NgkcieHdXIAAHAHIccmkcJj7uQAAOAOQo5NEtnWAQAAVxFybEJNDgAA7iLk2MSqySHkAADgCkKOTajJAQDAXYQcm1h7VzVSkwMAgBsIOTbxJlCTAwCAmwg5NqEmBwAAdxFybEJNDgAA7iLk2OT0FHJqcgAAcAMhxyaRx1VNYaMmNukEAMBxhBybRFY8lnhkBQCAGwg5NonU5EiEHAAA3NCikFNcXKxRo0YpJSVF6enpmjBhgiorK6PanDx5UoWFherWrZs6d+6s22+/XdXV1VFtqqqqVFBQoE6dOik9PV1z5sxRY2NjVJt169bpmmuukc/n04ABA7R06dKz+rN48WL16dNHSUlJysnJ0datW1vycWyVGHfmnRweVwEA4LQWhZz169ersLBQmzdvVklJiRoaGjR27FgdP37cajN79my9++67Wr58udavX6/9+/frtttus843NTWpoKBA9fX12rRpk15++WUtXbpU8+fPt9rs2bNHBQUFuummm1RRUaFZs2bp7rvv1urVq602b7zxhoqKivTwww/rww8/1LBhw5Sfn6+DBw9ezHi0mrg4jxLimGEFAIBrzEU4ePCgkWTWr19vjDHm6NGjJjEx0Sxfvtxq8+mnnxpJpqyszBhjzHvvvWfi4uJMIBCw2jz//PPG7/eburo6Y4wx999/vxkyZEjUe/34xz82+fn51s+jR482hYWF1s9NTU0mKyvLFBcXN7v/wWDQSDLBYLAFn7r5Bj74vuk9d4WpOnTclusDANARNff7+6JqcoLBoCSpa9eukqTy8nI1NDQoLy/PajNw4ED16tVLZWVlkqSysjINHTpUGRkZVpv8/HyFQiHt3LnTanPmNSJtIteor69XeXl5VJu4uDjl5eVZbc6lrq5OoVAo6rATa+UAAOCeCw454XBYs2bN0nXXXacrr7xSkhQIBOT1epWWlhbVNiMjQ4FAwGpzZsCJnI+c+3NtQqGQamtr9c0336ipqemcbSLXOJfi4mKlpqZaR3Z2dss/eAuc3tqBmhwAAJx2wSGnsLBQO3bs0Ouvv96a/bHVvHnzFAwGrWPfvn22vp+1SSd3cgAAcFzChfzSzJkztWLFCm3YsEE9e/a0Xs/MzFR9fb2OHj0adTenurpamZmZVpv/PQsqMvvqzDb/e0ZWdXW1/H6/kpOTFR8fr/j4+HO2iVzjXHw+n3w+X8s/8AVi/yoAANzTojs5xhjNnDlTb731ltasWaO+fftGnR8xYoQSExNVWlpqvVZZWamqqirl5uZKknJzc7V9+/aoWVAlJSXy+/0aPHiw1ebMa0TaRK7h9Xo1YsSIqDbhcFilpaVWm7bAqslpJOQAAOC0Ft3JKSws1Kuvvqp33nlHKSkpVv1LamqqkpOTlZqaqmnTpqmoqEhdu3aV3+/Xz372M+Xm5uraa6+VJI0dO1aDBw/WnXfeqUWLFikQCOjBBx9UYWGhdZdlxowZevbZZ3X//ffrrrvu0po1a7Rs2TKtXLnS6ktRUZGmTJmikSNHavTo0XryySd1/PhxTZ06tbXG5qIlsn8VAADuacmULUnnPF566SWrTW1trfnpT39qunTpYjp16mRuvfVWc+DAgajr7N2714wfP94kJyeb7t27m5///OemoaEhqs3atWvN8OHDjdfrNf369Yt6j4hnnnnG9OrVy3i9XjN69GizefPmlnwc26eQ3/LM70zvuSvMmk+rbbk+AAAdUXO/vz3GmA57myEUCik1NVXBYFB+v7/Vr3/785tU/scj+r93jlD+kPPXCgEAgOZr7vc3e1fZiHVyAABwDyHHRkwhBwDAPYQcG3kjIaexwz4RBADANYQcG7FODgAA7iHk2CgxgcdVAAC4hZBjIwqPAQBwDyHHRl4WAwQAwDWEHBtZNTls6wAAgOMIOTZiCjkAAO4h5NgoMYGaHAAA3ELIsRE1OQAAuIeQYyPWyQEAwD2EHBtZNTkUHgMA4DhCjo1YJwcAAPcQcmzkTaAmBwAAtxBybERNDgAA7iHk2Ih1cgAAcA8hx0bU5AAA4B5Cjo2sdXIaqckBAMBphBwbUZMDAIB7CDk2SkygJgcAALcQcmxETQ4AAO4h5NiIvasAAHAPIcdGVk0O2zoAAOA4Qo6NWCcHAAD3EHJs5E2gJgcAALcQcmyUSE0OAACuIeTYiHVyAABwDyHHRmfW5BjD3RwAAJxEyLFRZAq5MVJTmJADAICTCDk2Svy28FiiLgcAAKcRcmwUeVwlUZcDAIDTCDk2Sog7804OIQcAACcRcmzk8XjO2NqBkAMAgJMIOTazNulspCYHAAAnEXJslpjAWjkAALiBkGMz9q8CAMAdhBybUZMDAIA7CDk2s2pyCDkAADiKkGMza/8qCo8BAHAUIcdmCTyuAgDAFYQcm3l5XAUAgCsIOTZjdhUAAO4g5NjMqslhg04AABxFyLFZZDHAhkbu5AAA4CRCjs2oyQEAwB2EHJtRkwMAgDsIOTajJgcAAHcQcmzGnRwAANxByLGZN+HbmhwKjwEAcBQhx2bcyQEAwB2EHJtRkwMAgDsIOTbjTg4AAO5occjZsGGDbrnlFmVlZcnj8ejtt9+OOv+Tn/xEHo8n6hg3blxUm8OHD2vy5Mny+/1KS0vTtGnTdOzYsag2n3zyiW644QYlJSUpOztbixYtOqsvy5cv18CBA5WUlKShQ4fqvffea+nHsR3r5AAA4I4Wh5zjx49r2LBhWrx48XnbjBs3TgcOHLCO1157Ler85MmTtXPnTpWUlGjFihXasGGD7rnnHut8KBTS2LFj1bt3b5WXl+uxxx7TggUL9MILL1htNm3apEmTJmnatGn66KOPNGHCBE2YMEE7duxo6UeyFXdyAABwR0JLf2H8+PEaP378n23j8/mUmZl5znOffvqpVq1apW3btmnkyJGSpGeeeUY333yz/v3f/11ZWVl65ZVXVF9frxdffFFer1dDhgxRRUWFnnjiCSsMPfXUUxo3bpzmzJkjSXrkkUdUUlKiZ599VkuWLGnpx7JNZFuH+kZqcgAAcJItNTnr1q1Tenq6rrjiCt177706dOiQda6srExpaWlWwJGkvLw8xcXFacuWLVabG2+8UV6v12qTn5+vyspKHTlyxGqTl5cX9b75+fkqKys7b7/q6uoUCoWiDrtxJwcAAHe0esgZN26cfv3rX6u0tFT/9m//pvXr12v8+PFqamqSJAUCAaWnp0f9TkJCgrp27apAIGC1ycjIiGoT+fm72kTOn0txcbFSU1OtIzs7++I+bDNQkwMAgDta/Ljqu0ycONH689ChQ3XVVVepf//+WrduncaMGdPab9ci8+bNU1FRkfVzKBSyPehwJwcAAHfYPoW8X79+6t69uz7//HNJUmZmpg4ePBjVprGxUYcPH7bqeDIzM1VdXR3VJvLzd7U5Xy2QdKpWyO/3Rx12Y50cAADcYXvI+dOf/qRDhw6pR48ekqTc3FwdPXpU5eXlVps1a9YoHA4rJyfHarNhwwY1NDRYbUpKSnTFFVeoS5cuVpvS0tKo9yopKVFubq7dH6lFIoXHbOsAAICzWhxyjh07poqKClVUVEiS9uzZo4qKClVVVenYsWOaM2eONm/erL1796q0tFQ/+tGPNGDAAOXn50uSBg0apHHjxmn69OnaunWrfv/732vmzJmaOHGisrKyJEl33HGHvF6vpk2bpp07d+qNN97QU089FfWo6b777tOqVav0+OOPa/fu3VqwYIE++OADzZw5sxWGpfVQkwMAgEtMC61du9ZIOuuYMmWKOXHihBk7dqy59NJLTWJioundu7eZPn26CQQCUdc4dOiQmTRpkuncubPx+/1m6tSppqamJqrNxx9/bK6//nrj8/nMZZddZhYuXHhWX5YtW2Yuv/xy4/V6zZAhQ8zKlStb9FmCwaCRZILBYEuHodn+Z1fA9J67wvzVM7+z7T0AAOhImvv97THGdNhikVAopNTUVAWDQdvqczb84Wv93YtbNaiHX+/fd4Mt7wEAQEfS3O9v9q6yGbOrAABwByHHZt4EanIAAHADIcdm1p0cZlcBAOAoQo7NWCcHAAB3EHJsRk0OAADuIOTYzEvIAQDAFYQcmyVSeAwAgCsIOTY7/bjKqAMvSQQAgOMIOTaLhBzpVNABAADOIOTYzBsVcnhkBQCAUwg5Nkv8doNOiZADAICTCDk2i4/zyPNtzqkn5AAA4BhCjs08Hk9U8TEAAHAGIccBXrZ2AADAcYQcB0TqcqjJAQDAOYQcB5zev4qQAwCAUwg5DqAmBwAA5xFyHOBNYP8qAACcRshxgFWTQ+ExAACOIeQ4gJocAACcR8hxADU5AAA4j5DjAGudHO7kAADgGEKOAxITWCcHAACnEXIcYNXkUHgMAIBjCDkOoCYHAADnEXIcQE0OAADOI+Q4gL2rAABwHiHHAayTAwCA8wg5DkiMbOvQSE0OAABOIeQ4gJocAACcR8hxADU5AAA4j5DjAKaQAwDgPEKOAxJ5XAUAgOMIOQ7wJhByAABwGiHHAZGaHKaQAwDgHEKOA6jJAQDAeYQcB1ghhw06AQBwDCHHAayTAwCA8wg5DkhMoCYHAACnEXIckJQQL0mqrW9yuScAAHQchBwHdE5KkCQdq2t0uScAAHQchBwHpCQlSpJqThJyAABwCiHHAZ193MkBAMBphBwHpJzxuMoY1soBAMAJhBwHRO7kNIWNahsoPgYAwAmEHAd08sYr7tQsch2jLgcAAEcQchzg8Xisuzk11OUAAOAIQo5DIjOsuJMDAIAzCDkOse7kEHIAAHAEIcchp2dYNbjcEwAAOgZCjkMiqx5zJwcAAGcQchzCgoAAADiLkOOQFO7kAADgKEKOQ7iTAwCAs1occjZs2KBbbrlFWVlZ8ng8evvtt6POG2M0f/589ejRQ8nJycrLy9Nnn30W1ebw4cOaPHmy/H6/0tLSNG3aNB07diyqzSeffKIbbrhBSUlJys7O1qJFi87qy/LlyzVw4EAlJSVp6NCheu+991r6cRzDJp0AADirxSHn+PHjGjZsmBYvXnzO84sWLdLTTz+tJUuWaMuWLbrkkkuUn5+vkydPWm0mT56snTt3qqSkRCtWrNCGDRt0zz33WOdDoZDGjh2r3r17q7y8XI899pgWLFigF154wWqzadMmTZo0SdOmTdNHH32kCRMmaMKECdqxY0dLP5IjuJMDAIDDzEWQZN566y3r53A4bDIzM81jjz1mvXb06FHj8/nMa6+9ZowxZteuXUaS2bZtm9Xm/fffNx6Px3z11VfGGGOee+4506VLF1NXV2e1mTt3rrniiiusn//mb/7GFBQURPUnJyfH/P3f/32z+x8MBo0kEwwGm/07F+qNbVWm99wVZsqLW2x/LwAAYllzv79btSZnz549CgQCysvLs15LTU1VTk6OysrKJEllZWVKS0vTyJEjrTZ5eXmKi4vTli1brDY33nijvF6v1SY/P1+VlZU6cuSI1ebM94m0ibzPudTV1SkUCkUdTkmJ3MnhcRUAAI5o1ZATCAQkSRkZGVGvZ2RkWOcCgYDS09OjzickJKhr165Rbc51jTPf43xtIufPpbi4WKmpqdaRnZ3d0o94waxtHXhcBQCAIzrU7Kp58+YpGAxax759+xx7bxYDBADAWa0acjIzMyVJ1dXVUa9XV1db5zIzM3Xw4MGo842NjTp8+HBUm3Nd48z3OF+byPlz8fl88vv9UYdTTu9dxbYOAAA4oVVDTt++fZWZmanS0lLrtVAopC1btig3N1eSlJubq6NHj6q8vNxqs2bNGoXDYeXk5FhtNmzYoIaG04GgpKREV1xxhbp06WK1OfN9Im0i79PWnN67qlHGGJd7AwBA7GtxyDl27JgqKipUUVEh6VSxcUVFhaqqquTxeDRr1iz9y7/8i37zm99o+/bt+ru/+ztlZWVpwoQJkqRBgwZp3Lhxmj59urZu3arf//73mjlzpiZOnKisrCxJ0h133CGv16tp06Zp586deuONN/TUU0+pqKjI6sd9992nVatW6fHHH9fu3bu1YMECffDBB5o5c+bFj4oNIiEnbKTahiaXewMAQAfQ0mlba9euNZLOOqZMmWKMOTWN/KGHHjIZGRnG5/OZMWPGmMrKyqhrHDp0yEyaNMl07tzZ+P1+M3XqVFNTUxPV5uOPPzbXX3+98fl85rLLLjMLFy48qy/Lli0zl19+ufF6vWbIkCFm5cqVLfosTk4hD4fDpu8DK0zvuStMdbDW9vcDACBWNff722NMx312EgqFlJqaqmAw6Eh9zlULVit0slGlP/+B+l/a2fb3AwAgFjX3+7tDza5yG1s7AADgHEKOg6ziY0IOAAC2I+Q46PT+VUwjBwDAboQcB7EgIAAAziHkOOj0goCEHAAA7EbIcRD7VwEA4BxCjoPOXPUYAADYi5DjIB5XAQDgHEKOg9ikEwAA5xByHMTjKgAAnEPIcRCLAQIA4BxCjoM6+5hdBQCAUwg5DmIxQAAAnEPIcRCFxwAAOIeQ4yD/GYXHxhiXewMAQGwj5Dgo8rgqbKTahiaXewMAQGwj5DgoOTFecZ5Tf2aGFQAA9iLkOMjj8Vh1OSFCDgAAtiLkOIxNOgEAcAYhx2EsCAgAgDMIOQ6LPK46Vsc0cgAA7ETIcVhkhhU1OQAA2IuQ4zCrJoeQAwCArQg5Djv9uIqQAwCAnQg5DktJIuQAAOAEQo7D2L8KAABnEHIclsJO5AAAOIKQ4zBqcgAAcAYhx2EsBggAgDMIOQ7r7GNbBwAAnEDIcRg1OQAAOIOQ47DOScyuAgDACYQch6WcUXhsjHG5NwAAxC5CjsMid3LCRqptaHK5NwAAxC5CjsOSE+MVH+eRRF0OAAB2IuQ4zOPxnLHqMSEHAAC7EHJcwIKAAADYj5DjAhYEBADAfoQcF6QwjRwAANsRclxg1eTwuAoAANsQclzQOenbrR14XAUAgG0IOS6g8BgAAPsRclxATQ4AAPYj5LgghTs5AADYjpDjgs7sRA4AgO0IOS6gJgcAAPsRclzAYoAAANiPkOOClG+nkPO4CgAA+xByXMDjKgAA7EfIcUFnppADAGA7Qo4LzpxCboxxuTcAAMQmQo4LIjU5YSOdqG9yuTcAAMQmQo4LkhLjFB/nkURdDgAAdmn1kLNgwQJ5PJ6oY+DAgdb5kydPqrCwUN26dVPnzp11++23q7q6OuoaVVVVKigoUKdOnZSenq45c+aosTE6DKxbt07XXHONfD6fBgwYoKVLl7b2R7GNx+M5vRM5M6wAALCFLXdyhgwZogMHDljHxo0brXOzZ8/Wu+++q+XLl2v9+vXav3+/brvtNut8U1OTCgoKVF9fr02bNunll1/W0qVLNX/+fKvNnj17VFBQoJtuukkVFRWaNWuW7r77bq1evdqOj2MLZlgBAGCvBFsumpCgzMzMs14PBoP6r//6L7366qv6i7/4C0nSSy+9pEGDBmnz5s269tpr9dvf/la7du3S//zP/ygjI0PDhw/XI488orlz52rBggXyer1asmSJ+vbtq8cff1ySNGjQIG3cuFG/+tWvlJ+fb8dHanVs0gkAgL1suZPz2WefKSsrS/369dPkyZNVVVUlSSovL1dDQ4Py8vKstgMHDlSvXr1UVlYmSSorK9PQoUOVkZFhtcnPz1coFNLOnTutNmdeI9Imco3zqaurUygUijrccmmKT5IUCJ50rQ8AAMSyVg85OTk5Wrp0qVatWqXnn39ee/bs0Q033KCamhoFAgF5vV6lpaVF/U5GRoYCgYAkKRAIRAWcyPnIuT/XJhQKqba29rx9Ky4uVmpqqnVkZ2df7Me9YNldO0mS9h05f38BAMCFa/XHVePHj7f+fNVVVyknJ0e9e/fWsmXLlJyc3Npv1yLz5s1TUVGR9XMoFHIt6GR3+TbkHD7hyvsDABDrbJ9CnpaWpssvv1yff/65MjMzVV9fr6NHj0a1qa6utmp4MjMzz5ptFfn5u9r4/f4/G6R8Pp/8fn/U4ZZe397JqSLkAABgC9tDzrFjx/TFF1+oR48eGjFihBITE1VaWmqdr6ysVFVVlXJzcyVJubm52r59uw4ePGi1KSkpkd/v1+DBg602Z14j0iZyjfaAkAMAgL1aPeT84z/+o9avX6+9e/dq06ZNuvXWWxUfH69JkyYpNTVV06ZNU1FRkdauXavy8nJNnTpVubm5uvbaayVJY8eO1eDBg3XnnXfq448/1urVq/Xggw+qsLBQPt+pYt0ZM2boyy+/1P3336/du3frueee07JlyzR79uzW/ji2iYScr2vqVMuqxwAAtLpWr8n505/+pEmTJunQoUO69NJLdf3112vz5s269NJLJUm/+tWvFBcXp9tvv111dXXKz8/Xc889Z/1+fHy8VqxYoXvvvVe5ubm65JJLNGXKFP3yl7+02vTt21crV67U7Nmz9dRTT6lnz576z//8z3YzfVySUjslyp+UoNDJRu07ckKXZ6S43SUAAGKKx3TgHSJDoZBSU1MVDAZdqc8pePp32rk/pP+aMlJjBmV89y8AAIBmf3+zd5WLqMsBAMA+hBwXEXIAALAPIcdF1oKAhBwAAFodIcdF3MkBAMA+hBwX9bLu5NSqA9d/AwBgC0KOi7LSkuXxSLUNTfrmWL3b3QEAIKYQclzkTYhTVuqpbSh4ZAUAQOsi5Lgsu+upkEPxMQAArYuQ4zKKjwEAsAchx2WEHAAA7EHIcVk2IQcAAFsQclzGgoAAANiDkOOyyOOqQOik6hqbXO4NAACxg5Djsm6XeNXJGy9jpK+O1LrdHQAAYgYhx2Uej4fiYwAAbEDIaQOoywEAoPURctqA7C7cyQEAoLURctqAXl3Z2gEAgNZGyGkDenU7vRs5AABoHYScNqDXGTU5xhiXewMAQGwg5LQBPb+tyampa9TREw0u9wYAgNhAyGkDkhLjleH3SaIuBwCA1kLIaSOYYQUAQOsi5LQRLAgIAEDrIuS0EZEFAf90hJADAEBrIOS0EdzJAQCgdRFy2og+3S+RJO3aH1JTmGnkAABcLEJOG3FVz1SlJifqyIkGlf/xiNvdAQCg3SPktBGJ8XEaMzBdkvTbnQGXewMAQPtHyGlDxg7JkCSt3hVg5WMAAC4SIacNufHyS+VLiNO+w7XaHahxuzsAALRrhJw2pJM3QTd871JJ0m93VrvcGwAA2jdCThtjPbKiLgcAgItCyGlj8gZlKM4j7ToQ0j7WzAEA4IIRctqYrpd4NapPV0lSyS4eWQEAcKEIOW3Q2CGZknhkBQDAxSDktEFjB5+qy9m297AOH693uTcAALRPhJw2KLtrJw3u4VfYSKWf8sgKAIALQchpo/KtR1aEHAAALgQhp42KTCX/3Wdf60R9o8u9AQCg/SHktFEDM1OU3TVZdY1hvfvxfre7AwBAu0PIaaM8Ho8mje4lSfrlu7u055vjLvcIAID2hZDThv39jf2V07erjtc3qfCVD3WyocntLgEA0G4Qctqw+DiPnp50tbpd4tWuAyH9y8pdbncJAIB2g5DTxmX4k/TEj4dLkv57c5VWfEJ9DgAAzUHIaQd+cPml+ukP+0uSHvh/2/XHQ9TnAADwXQg57UTR/7lco/p00bG6Rk15cavWVR50u0sAALRphJx2IiE+Tk9PulrpKT7tPXRCP3lpm6a8uFWfVde43TUAANokjzHGuN0Jt4RCIaWmpioYDMrv97vdnWYJnmjQ02s+06/L9qqhySg+zqOJo7I14erLdFXPVPkS4t3uIgAAtmru9zchp52FnIg93xzXwvc/jdr2wZsQp+E90zSqbxcNyUpVhj9J6Sk+pft9hB8AQMwg5DRDew45EWVfHNKvy/Zq297D+ubY+XcsT01OVGdfgjp549XJG69kb7ySEuOVEOdRQlyc4uM9SojzKN7jkcfjUZxHivN45PFIHo8kfftnRX6O5lH0i+dq0xwX+GtwkedC/2cDaFcu9K960f+5XClJia3al+Z+fye06rvCcbn9uym3fzcZY7Tnm+Patvewtu45or2Hjqs6dFIHQ3WqbworWNugYG2D290FAHQw9/6wf6uHnOYi5MQIj8ejfpd2Vr9LO+vHo3pZrxtjFKxt0Nc1dTpW16ja+iadqG9SbUOTTjY0qSls1BA2amoKqzFsFDZGYSOFjZExp37fGMlI3/7XnHHtb//bnA523BuGF4wR6zj469FxmA74N7uT172oQciJcR6PR2mdvErr5HW7KwAAOKrdTyFfvHix+vTpo6SkJOXk5Gjr1q1udwkAALQB7TrkvPHGGyoqKtLDDz+sDz/8UMOGDVN+fr4OHmShPAAAOrp2HXKeeOIJTZ8+XVOnTtXgwYO1ZMkSderUSS+++KLbXQMAAC5rtyGnvr5e5eXlysvLs16Li4tTXl6eysrKzvk7dXV1CoVCUQcAAIhN7TbkfPPNN2pqalJGRkbU6xkZGQoEAuf8neLiYqWmplpHdna2E10FAAAuaLch50LMmzdPwWDQOvbt2+d2lwAAgE3a7RTy7t27Kz4+XtXV1VGvV1dXKzMz85y/4/P55PP5nOgeAABwWbu9k+P1ejVixAiVlpZar4XDYZWWlio3N9fFngEAgLag3d7JkaSioiJNmTJFI0eO1OjRo/Xkk0/q+PHjmjp1qttdAwAALmvXIefHP/6xvv76a82fP1+BQEDDhw/XqlWrzipGBgAAHQ+7kLfzXcgBAOhomvv93W5rcgAAAP4cQg4AAIhJ7bom52JFntSx8jEAAO1H5Hv7uypuOnTIqampkSRWPgYAoB2qqalRamrqec936MLjcDis/fv3KyUlRR6Pp9WuGwqFlJ2drX379lHQbDPG2jmMtXMYa2cx3s5prbE2xqimpkZZWVmKizt/5U2HvpMTFxennj172nZ9v9/PXxiHMNbOYaydw1g7i/F2TmuM9Z+7gxNB4TEAAIhJhBwAABCTCDk28Pl8evjhh9kM1AGMtXMYa+cw1s5ivJ3j9Fh36MJjAAAQu7iTAwAAYhIhBwAAxCRCDgAAiEmEHAAAEJMIOTZYvHix+vTpo6SkJOXk5Gjr1q1ud6ldKy4u1qhRo5SSkqL09HRNmDBBlZWVUW1OnjypwsJCdevWTZ07d9btt9+u6upql3ocOxYuXCiPx6NZs2ZZrzHWreurr77S3/7t36pbt25KTk7W0KFD9cEHH1jnjTGaP3++evTooeTkZOXl5emzzz5zscftU1NTkx566CH17dtXycnJ6t+/vx555JGovY8Y6wuzYcMG3XLLLcrKypLH49Hbb78ddb4543r48GFNnjxZfr9faWlpmjZtmo4dO3bxnTNoVa+//rrxer3mxRdfNDt37jTTp083aWlpprq62u2utVv5+fnmpZdeMjt27DAVFRXm5ptvNr169TLHjh2z2syYMcNkZ2eb0tJS88EHH5hrr73WfP/733ex1+3f1q1bTZ8+fcxVV11l7rvvPut1xrr1HD582PTu3dv85Cc/MVu2bDFffvmlWb16tfn888+tNgsXLjSpqanm7bffNh9//LH5q7/6K9O3b19TW1vrYs/bn0cffdR069bNrFixwuzZs8csX77cdO7c2Tz11FNWG8b6wrz33nvmF7/4hXnzzTeNJPPWW29FnW/OuI4bN84MGzbMbN682fzud78zAwYMMJMmTbrovhFyWtno0aNNYWGh9XNTU5PJysoyxcXFLvYqthw8eNBIMuvXrzfGGHP06FGTmJholi9fbrX59NNPjSRTVlbmVjfbtZqaGvO9733PlJSUmB/84AdWyGGsW9fcuXPN9ddff97z4XDYZGZmmscee8x67ejRo8bn85nXXnvNiS7GjIKCAnPXXXdFvXbbbbeZyZMnG2MY69byv0NOc8Z1165dRpLZtm2b1eb99983Ho/HfPXVVxfVHx5XtaL6+nqVl5crLy/Pei0uLk55eXkqKytzsWexJRgMSpK6du0qSSovL1dDQ0PUuA8cOFC9evVi3C9QYWGhCgoKosZUYqxb229+8xuNHDlSf/3Xf6309HRdffXV+o//+A/r/J49exQIBKLGOzU1VTk5OYx3C33/+99XaWmp/vCHP0iSPv74Y23cuFHjx4+XxFjbpTnjWlZWprS0NI0cOdJqk5eXp7i4OG3ZsuWi3r9Db9DZ2r755hs1NTUpIyMj6vWMjAzt3r3bpV7FlnA4rFmzZum6667TlVdeKUkKBALyer1KS0uLapuRkaFAIOBCL9u3119/XR9++KG2bdt21jnGunV9+eWXev7551VUVKR/+qd/0rZt2/QP//AP8nq9mjJlijWm5/o3hfFumQceeEChUEgDBw5UfHy8mpqa9Oijj2ry5MmSxFjbpDnjGggElJ6eHnU+ISFBXbt2veixJ+SgXSksLNSOHTu0ceNGt7sSk/bt26f77rtPJSUlSkpKcrs7MS8cDmvkyJH613/9V0nS1VdfrR07dmjJkiWaMmWKy72LLcuWLdMrr7yiV199VUOGDFFFRYVmzZqlrKwsxjqG8biqFXXv3l3x8fFnzTSprq5WZmamS72KHTNnztSKFSu0du1a9ezZ03o9MzNT9fX1Onr0aFR7xr3lysvLdfDgQV1zzTVKSEhQQkKC1q9fr6effloJCQnKyMhgrFtRjx49NHjw4KjXBg0apKqqKkmyxpR/Uy7enDlz9MADD2jixIkaOnSo7rzzTs2ePVvFxcWSGGu7NGdcMzMzdfDgwajzjY2NOnz48EWPPSGnFXm9Xo0YMUKlpaXWa+FwWKWlpcrNzXWxZ+2bMUYzZ87UW2+9pTVr1qhv375R50eMGKHExMSoca+srFRVVRXj3kJjxozR9u3bVVFRYR0jR47U5MmTrT8z1q3nuuuuO2s5hD/84Q/q3bu3JKlv377KzMyMGu9QKKQtW7Yw3i104sQJxcVFf+XFx8crHA5LYqzt0pxxzc3N1dGjR1VeXm61WbNmjcLhsHJyci6uAxdVtoyzvP7668bn85mlS5eaXbt2mXvuucekpaWZQCDgdtfarXvvvdekpqaadevWmQMHDljHiRMnrDYzZswwvXr1MmvWrDEffPCByc3NNbm5uS72OnacObvKGMa6NW3dutUkJCSYRx991Hz22WfmlVdeMZ06dTL//d//bbVZuHChSUtLM++884755JNPzI9+9COmNV+AKVOmmMsuu8yaQv7mm2+a7t27m/vvv99qw1hfmJqaGvPRRx+Zjz76yEgyTzzxhPnoo4/MH//4R2NM88Z13Lhx5uqrrzZbtmwxGzduNN/73veYQt5WPfPMM6ZXr17G6/Wa0aNHm82bN7vdpXZN0jmPl156yWpTW1trfvrTn5ouXbqYTp06mVtvvdUcOHDAvU7HkP8dchjr1vXuu++aK6+80vh8PjNw4EDzwgsvRJ0Ph8PmoYceMhkZGcbn85kxY8aYyspKl3rbfoVCIXPfffeZXr16maSkJNOvXz/zi1/8wtTV1VltGOsLs3bt2nP+Gz1lyhRjTPPG9dChQ2bSpEmmc+fOxu/3m6lTp5qampqL7pvHmDOWewQAAIgR1OQAAICYRMgBAAAxiZADAABiEiEHAADEJEIOAACISYQcAAAQkwg5AAAgJhFyAABATCLkAACAmETIAQAAMYmQAwAAYhIhBwAAxKT/D/5U9fBlRSCeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Q3\n",
    "x=torch.tensor([5.0,7.0,12.0,16.0,20.0])\n",
    "y=torch.tensor([40.0,120.0,180.0,210.0,240.0])\n",
    "lerning_rate=torch.tensor(0.001)\n",
    "\n",
    "class RegressionModel:\n",
    "    def __init__(self):\n",
    "        self.w=torch.rand([1],requires_grad=True)\n",
    "        self.b=torch.rand([1],requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        return self.w*x+self.b\n",
    "    def update(self):\n",
    "        self.w-=learning_rate*self.w.grad\n",
    "        self.b-=learning_rate*self.b.grad\n",
    "    def reset_grad(self):\n",
    "        self.w.grad.zero_()\n",
    "        self.b.grad.zero_()\n",
    "\n",
    "def criterion(yj,yp):\n",
    "    return (yp-yj)**2\n",
    "    \n",
    "model=RegressionModel()\n",
    "loss_list=[]\n",
    "for epoch in range(100):\n",
    "    loss=0.0\n",
    "    for j in range(len(x)):\n",
    "        y_pred=model.forward(x[j])\n",
    "        loss+=criterion(y[j],y_pred)\n",
    "    loss=loss/len(x)\n",
    "    loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        model.update()\n",
    "    model.reset_grad()\n",
    "    print(f\"Epoch= {epoch+1} : The parameters are w= {w}, b= {b}, and loss= {loss.item()}\")\n",
    "plt.plot(loss_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c40d9d8-00bc-40fd-b6f0-1979812357b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23]) torch.Size([23])\n",
      "Epoch = 0, loss= 493.4317321777344\n",
      "Epoch = 1, loss= 434.0224914550781\n",
      "Epoch = 2, loss= 381.7660827636719\n",
      "Epoch = 3, loss= 335.8013610839844\n",
      "Epoch = 4, loss= 295.37078857421875\n",
      "Epoch = 5, loss= 259.8081359863281\n",
      "Epoch = 6, loss= 228.5271759033203\n",
      "Epoch = 7, loss= 201.01243591308594\n",
      "Epoch = 8, loss= 176.81053161621094\n",
      "Epoch = 9, loss= 155.52249145507812\n",
      "Epoch = 10, loss= 136.7975616455078\n",
      "Epoch = 11, loss= 120.32710266113281\n",
      "Epoch = 12, loss= 105.83972930908203\n",
      "Epoch = 13, loss= 93.0965805053711\n",
      "Epoch = 14, loss= 81.88773345947266\n",
      "Epoch = 15, loss= 72.02843475341797\n",
      "Epoch = 16, loss= 63.356201171875\n",
      "Epoch = 17, loss= 55.72809982299805\n",
      "Epoch = 18, loss= 49.018428802490234\n",
      "Epoch = 19, loss= 43.116600036621094\n",
      "Epoch = 20, loss= 37.925357818603516\n",
      "Epoch = 21, loss= 33.359130859375\n",
      "Epoch = 22, loss= 29.34268569946289\n",
      "Epoch = 23, loss= 25.8098201751709\n",
      "Epoch = 24, loss= 22.702316284179688\n",
      "Epoch = 25, loss= 19.968950271606445\n",
      "Epoch = 26, loss= 17.564685821533203\n",
      "Epoch = 27, loss= 15.449893951416016\n",
      "Epoch = 28, loss= 13.58972454071045\n",
      "Epoch = 29, loss= 11.953521728515625\n",
      "Epoch = 30, loss= 10.514312744140625\n",
      "Epoch = 31, loss= 9.248390197753906\n",
      "Epoch = 32, loss= 8.134879112243652\n",
      "Epoch = 33, loss= 7.155439853668213\n",
      "Epoch = 34, loss= 6.293923377990723\n",
      "Epoch = 35, loss= 5.536134243011475\n",
      "Epoch = 36, loss= 4.869581699371338\n",
      "Epoch = 37, loss= 4.28328275680542\n",
      "Epoch = 38, loss= 3.7675740718841553\n",
      "Epoch = 39, loss= 3.313958168029785\n",
      "Epoch = 40, loss= 2.914957046508789\n",
      "Epoch = 41, loss= 2.563995599746704\n",
      "Epoch = 42, loss= 2.2552900314331055\n",
      "Epoch = 43, loss= 1.983751893043518\n",
      "Epoch = 44, loss= 1.7449082136154175\n",
      "Epoch = 45, loss= 1.5348212718963623\n",
      "Epoch = 46, loss= 1.3500277996063232\n",
      "Epoch = 47, loss= 1.1874840259552002\n",
      "Epoch = 48, loss= 1.0445111989974976\n",
      "Epoch = 49, loss= 0.9187518954277039\n",
      "Epoch = 50, loss= 0.8081337213516235\n",
      "Epoch = 51, loss= 0.7108344435691833\n",
      "Epoch = 52, loss= 0.6252500414848328\n",
      "Epoch = 53, loss= 0.5499691963195801\n",
      "Epoch = 54, loss= 0.4837532341480255\n",
      "Epoch = 55, loss= 0.42550912499427795\n",
      "Epoch = 56, loss= 0.37427785992622375\n",
      "Epoch = 57, loss= 0.3292143642902374\n",
      "Epoch = 58, loss= 0.2895771861076355\n",
      "Epoch = 59, loss= 0.2547118067741394\n",
      "Epoch = 60, loss= 0.22404438257217407\n",
      "Epoch = 61, loss= 0.19706936180591583\n",
      "Epoch = 62, loss= 0.17334219813346863\n",
      "Epoch = 63, loss= 0.1524718701839447\n",
      "Epoch = 64, loss= 0.13411389291286469\n",
      "Epoch = 65, loss= 0.1179668977856636\n",
      "Epoch = 66, loss= 0.10376328974962234\n",
      "Epoch = 67, loss= 0.09127029031515121\n",
      "Epoch = 68, loss= 0.08028137683868408\n",
      "Epoch = 69, loss= 0.07061567157506943\n",
      "Epoch = 70, loss= 0.0621134415268898\n",
      "Epoch = 71, loss= 0.05463505908846855\n",
      "Epoch = 72, loss= 0.048056960105895996\n",
      "Epoch = 73, loss= 0.04227098822593689\n",
      "Epoch = 74, loss= 0.037181556224823\n",
      "Epoch = 75, loss= 0.03270480036735535\n",
      "Epoch = 76, loss= 0.028767108917236328\n",
      "Epoch = 77, loss= 0.02530350349843502\n",
      "Epoch = 78, loss= 0.022256996482610703\n",
      "Epoch = 79, loss= 0.01957736723124981\n",
      "Epoch = 80, loss= 0.01722019724547863\n",
      "Epoch = 81, loss= 0.015146908350288868\n",
      "Epoch = 82, loss= 0.013323218561708927\n",
      "Epoch = 83, loss= 0.01171904243528843\n",
      "Epoch = 84, loss= 0.010308046825230122\n",
      "Epoch = 85, loss= 0.009066975675523281\n",
      "Epoch = 86, loss= 0.007975318469107151\n",
      "Epoch = 87, loss= 0.0070151155814528465\n",
      "Epoch = 88, loss= 0.00617046607658267\n",
      "Epoch = 89, loss= 0.005427578929811716\n",
      "Epoch = 90, loss= 0.004774110391736031\n",
      "Epoch = 91, loss= 0.004199280869215727\n",
      "Epoch = 92, loss= 0.003693670965731144\n",
      "Epoch = 93, loss= 0.0032489525619894266\n",
      "Epoch = 94, loss= 0.002857782645151019\n",
      "Epoch = 95, loss= 0.0025137297343462706\n",
      "Epoch = 96, loss= 0.0022110778372734785\n",
      "Epoch = 97, loss= 0.0019448549719527364\n",
      "Epoch = 98, loss= 0.0017107047606259584\n",
      "Epoch = 99, loss= 0.0015047276392579079\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "x=torch.tensor([12.4,14.3,14.5,14.9,16.1,16.9,16.5,15.4,17.0,17.9,18.8,20.3,22.4,19.4,15.5,16.7,17.3,18.4,19.2,17.4,19.5,19.7,21.2])\n",
    "y=torch.tensor([11.2,12.5,12.7,13.1,14.1,14.8,14.4,13.4,14.9,15.6,16.4,17.7,19.6,16.9,14.0,14.6,15.1,16.1,16.8,15.2,17.0,17.2,18.6])\n",
    "print(x.shape,y.shape)\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear=torch.nn.Linear(23,23)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "model = RegressionModel()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "for epochs in range(100):\n",
    "    pred_y=model(x)\n",
    "    loss=criterion(pred_y,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch = {epochs}, loss= {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142a3107-e8f6-4289-8172-ebbbcd304e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, The parameters are w=Parameter containing:\n",
      "tensor([0.6645], requires_grad=True),b=Parameter containing:\n",
      "tensor([0.7179], requires_grad=True), and loss =Parameter containing:\n",
      "tensor([0.3540], requires_grad=True)\n",
      "After 33 epochs, The parameters are w=Parameter containing:\n",
      "tensor([1.3719], requires_grad=True),b=Parameter containing:\n",
      "tensor([-0.5982], requires_grad=True), and loss =Parameter containing:\n",
      "tensor([0.4518], requires_grad=True)\n",
      "After 66 epochs, The parameters are w=Parameter containing:\n",
      "tensor([1.9937], requires_grad=True),b=Parameter containing:\n",
      "tensor([-1.0464], requires_grad=True), and loss =Parameter containing:\n",
      "tensor([0.5790], requires_grad=True)\n",
      "After 99 epochs, The parameters are w=Parameter containing:\n",
      "tensor([2.2790], requires_grad=True),b=Parameter containing:\n",
      "tensor([-1.2943], requires_grad=True), and loss =Parameter containing:\n",
      "tensor([0.6543], requires_grad=True)\n",
      "tensor(1.4910, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,X1,X2,Y):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return len(self.X1)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X1[idx],self.X2[idx],self.Y[idx]\n",
    "\n",
    "x1 = torch.tensor([3,4,5,6,2])\n",
    "x2 = torch.tensor([8,5,7,3,1])\n",
    "y = torch.tensor([-3.5,3.5,2.5,11.5,5.7])\n",
    "dataset = MyDataset(x1,x2,y)\n",
    "data_loader = DataLoader(dataset,batch_size=2,shuffle=True)\n",
    "\n",
    "\n",
    "class RegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.w2 = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "        self.b = torch.nn.Parameter(torch.rand([1],requires_grad = True))\n",
    "    def forward(self,x1,x2):\n",
    "        return self.w1*x1 + self.w2*x2 + self.b\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "model = RegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001)\n",
    "\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        x1,x2,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x1,x2)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss += loss.item()\n",
    "\n",
    "    if epochs%33==0:\n",
    "        print(\"After {} epochs, The parameters are w={},b={}, and loss ={}\".format(epochs, model.w1,model.w2,model.b,loss.item()))\n",
    "\n",
    "    finalloss = loss/len(data_loader)*4\n",
    "\n",
    "print(finalloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899484b-1845-4d35-9e16-7e3d98e795f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
