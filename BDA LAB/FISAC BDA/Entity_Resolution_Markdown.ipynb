{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ed34b4",
   "metadata": {},
   "source": [
    "<h2>Entity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7fb9d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This code snippet sets up a PySpark environment in a Python script.</li><li> It first imports the necessary modules like pyspark, os, and sys.</li><li> Then, it sets the Python executable for PySpark to the same one being used by the script.</li><li> Finally, it imports the SparkContext class for creating RDDs and the SparkSession class for programming Spark with the DataFrame API.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c340fd",
   "metadata": {},
   "source": [
    "<ul><li>This code creates a SparkSession named spark with specific configuration options. It sets the driver memory to 16 GB and names the application 'chapter_2'.</li><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea97c",
   "metadata": {},
   "source": [
    "<h3>Setting up our data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25960bd1",
   "metadata": {},
   "source": [
    "<ul><li>This shell script creates a directory named \"linkage,\" navigates into it, downloads a file named \"donation.zip\" using curl, and then unzips it. Finally, it unzips all files matching the pattern \"block_*.zip\" within the \"linkage\" directory.</li></ul>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4220800e",
   "metadata": {},
   "source": [
    "From the shell:\n",
    " $ mkdir linkage\n",
    " $ cd linkage/\n",
    " $ curl-L-o donation.zip https://bit.ly/1Aoywaq\n",
    " $ unzip donation.zip\n",
    " $ unzip 'block_*.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f0458",
   "metadata": {},
   "source": [
    "<ul><li>This code reads a CSV file named \"block_1.csv\" located in the \"data/linkage/donation/block_1\" directory into a Spark DataFrame called prev. It then displays the first two rows of the DataFrame using show(2) and subsequently displays the entire DataFrame prev.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "prev.show(2)\n",
    "prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb01f6",
   "metadata": {},
   "source": [
    "<ul><li>This code reads a CSV file named \"block_1.csv\" into a Spark DataFrame called parsed. Further displaying its schema.</ul></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed =spark.read.option(\"header\",\"true\").option(\"nullValue\", \"?\").\\option(\"inferSchema\", \"true\").csv(\"data/linkage/donation/block_1/block_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999d3db",
   "metadata": {},
   "source": [
    "<ul><li>This code first calculates the number of rows in the parsed DataFrame. It then caches the parsed DataFrame in memory for faster access in subsequent operations.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0004878",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.printSchema()\n",
    "parsed.show(5)\n",
    "parsed.count()\n",
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1330e",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet groups the parsed DataFrame by the \"is_match\" column and calculates the count of each group. It then orders the result in descending order based on the count of each group and displays the result, showing the number of occurrences for each \"is_match\" value, with the most frequent ones listed first.</ul></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f64460",
   "metadata": {},
   "source": [
    "<ul><li>This code creates a temporary view called \"linkage\" from the parsed DataFrame. This temporary view allows you to query the DataFrame using SQL syntax.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd409eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cefdc6",
   "metadata": {},
   "source": [
    "<ul><li>This Spark SQL query selects the \"is_match\" column and counts the number of occurrences for each value. It groups the results by \"is_match\" and orders them in descending order based on the count. Finally, it displays the \"is_match\" values along with their respective counts.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT is_match, COUNT(*) cnt\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "ORDER BY cnt DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095e6ae",
   "metadata": {},
   "source": [
    "<h3>Fast Summary Statistics for DataFrames</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179104cd",
   "metadata": {},
   "source": [
    "<b>This code computes summary statistics for specific columns in the parsed DataFrame.\n",
    "</b>\n",
    "<ul><li>summary = parsed.describe() calculates summary statistics for all columns in parsed and selects and displays summary statistics for the \"cmp_fname_c1\" and \"cmp_fname_c2\" columns from summary.</li></ul>\n",
    "<b>\n",
    "It then separates the data into two DataFrames based on the \"is_match\" column.</b>\n",
    "<ul><li>\n",
    "matches = parsed.where(\"is_match = true\") filters parsed to include only rows where \"is_match\" is true.\n",
    "    </li><li>\n",
    "match_summary = matches.describe() computes summary statistics for matches.\n",
    "    </li><li>\n",
    "misses = parsed.filter(col(\"is_match\") == False) filters parsed to include only rows where \"is_match\" is false.\n",
    "    </li><li>\n",
    "miss_summary = misses.describe() computes summary statistics for misses.\n",
    "    </li></ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a540be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = parsed.describe()\n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n",
    "\n",
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93da3b",
   "metadata": {},
   "source": [
    "<h3>Pivoting and Reshaping Dataframes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba728014",
   "metadata": {},
   "source": [
    "<ul><li>This code converts summary statistics from a PySpark DataFrame to a Pandas DataFrame for easier analysis in Python. It reshapes the Pandas DataFrame by transposing it and resetting the index, making it easier to work with. After renaming columns and removing the axis name, the Pandas DataFrame is converted back to a Spark DataFrame for further processing.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()\n",
    "summary_p.head()\n",
    "summary_p.shape\n",
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "summary_p.shape\n",
    "summaryT = spark.createDataFrame(summary_p)\n",
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524464d7",
   "metadata": {},
   "source": [
    "<ul><li>This function pivot_summary(desc) converts summary statistics from a PySpark DataFrame desc to a more manageable format by first converting it to a Pandas DataFrame.</li><li> It then transposes the Pandas DataFrame, sets the 'summary' column as the index, and resets the index.</li><li> After renaming columns and removing the axis name, the Pandas DataFrame is converted back to a Spark DataFrame descT.</li><li> Finally, the function converts the metric columns to DoubleType from string format before returning the modified Spark DataFrame descT.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "def pivot_summary(desc):\n",
    "    desc_p = desc.toPandas()\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "        return descT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c72da",
   "metadata": {},
   "source": [
    "<ul><li>It uses a function pivot_summary that converts each DataFrame to a Pandas DataFrame, transposes it, resets the index, renames columns, and then converts it back to a Spark DataFrame (match_summaryT and miss_summaryT). </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf62ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2c8cb",
   "metadata": {},
   "source": [
    "<h3>Joining DataFrames and Selecting Features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d23523",
   "metadata": {},
   "source": [
    "<ul><li>This code creates temporary views for the match_summaryT DataFrame as \"match_desc\" and the miss_summaryT DataFrame as \"miss_desc\". </li><li>It then performs a SQL query that joins these views on the \"field\" column, calculates the total count for each field by adding the counts from the match and miss DataFrames, and computes the difference in means (delta) between the match and miss DataFrames for each field.</li><li> It filters out fields \"id_1\" and \"id_2\" and orders the results by delta in descending order, followed by total in descending order.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.field, a.count + b.count total, a.mean- b.mean delta\n",
    "FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "ORDER BY delta DESC, total DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2100aa",
   "metadata": {},
   "source": [
    "<h3>Scoring and Model Evaluation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65885cc6",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet defines a list of good features named good_features that are selected for further analysis.</li><li> It then creates a sum expression by joining these features with the \"+\" operator, which can be used in mathematical operations.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "sum_expression = \" + \".join(good_features)\n",
    "sum_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0ee3b",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet uses the good_features list to fill null values in the parsed DataFrame with 0 for the selected features.</li><li> It then calculates a new column 'score' by summing up the values of the selected features using the expr function with the sum_expression. </li><li>Finally, it selects the 'score' and 'is_match' columns from the DataFrame and displays the result using the show method.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "scored = parsed.fillna(0, subset=good_features).\\\n",
    "withColumn('score', expr(sum_expression)).\\\n",
    "select('score', 'is_match')\n",
    "scored.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028a71f",
   "metadata": {},
   "source": [
    "<ul><li>This function computes a cross-tabulation between two columns, 'above' (indicating if a score is above a threshold) and 'is_match'. It groups the data by 'above' and 'is_match' values, then counts the occurrences of 'true' and 'false' in 'is_match' for each 'above' value.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45224714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossTabs(scored: DataFrame, t: DoubleType)-> DataFrame:\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1f609",
   "metadata": {},
   "source": [
    "<ul><li>It computes a cross-tabulation between the 'above' column (indicating if the score is above or equal to 4.0) and the 'is_match' column. </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
