{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85869e75",
   "metadata": {},
   "source": [
    "<h2>Decision Trees</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c6cc8",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This code snippet sets up a PySpark environment in a Python script.</li><li> It first imports the necessary modules like pyspark, os, and sys.</li><li> Then, it sets the Python executable for PySpark to the same one being used by the script.</li><li> Finally, it imports the SparkContext class for creating RDDs and the SparkSession class for programming Spark with the DataFrame API.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a0a94",
   "metadata": {},
   "source": [
    "<ul><li>This code creates a SparkSession named spark with specific configuration options. It sets the driver memory to 16 GB and names the application 'chapter_4'.</li><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb7ce6",
   "metadata": {},
   "source": [
    "<h3>Preparing Data</h3>\n",
    "<ul><li>This code reads a CSV file named \"covtype.data\" without a header into a PySpark DataFrame called data_without_header. It then prints the schema of this DataFrame, allowing users to inspect the structure of the data.</ul></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True)\\.option(\"header\", False).csv(\"data/covtype.data\")\n",
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5524c",
   "metadata": {},
   "source": [
    "<ul><li>This code prepares a PySpark DataFrame named data by defining a list of column names (colnames) expected in the dataset. </li><li>These columns represent various geographical features and identifiers.</li><li> The data_without_header DataFrame is then converted to data using these column names, and the \"Cover_Type\" column is cast to a DoubleType to ensure it is treated as a numerical column.</li><li> Finally, it returns the first row of the DataFrame, providing a glimpse into the data's structure and content.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b881e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    " colnames = [\"Elevation\", \"Aspect\", \"Slope\", \\\n",
    " \"Horizontal_Distance_To_Hydrology\", \\\n",
    " \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \\\n",
    " \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \\\n",
    " \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "[f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    " [f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    " [\"Cover_Type\"]\n",
    "data = data_without_header.toDF(*colnames).\\\n",
    " withColumn(\"Cover_Type\",col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21796f2",
   "metadata": {},
   "source": [
    "<h3>Our First Decision Tree</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b1336",
   "metadata": {},
   "source": [
    "<ul><li>This code splits the data DataFrame into train_data and test_data DataFrames using a 90-10 ratio </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfc791",
   "metadata": {},
   "source": [
    "<b>This code uses PySpark's VectorAssembler to merge multiple feature columns from train_data into a single \"featureVector\" column.</b>\n",
    "<ul>\n",
    "    <li>It excludes the last column (\"Cover_Type\") from the feature set.</li>\n",
    "<li>The transform method is applied to train_data, creating assembled_train_data with the new \"featureVector\" column.</li>\n",
    "<li>The final select statement displays the \"featureVector\" column content for each row in assembled_train_data.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols,outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833cab8",
   "metadata": {},
   "source": [
    "<b>The code trains a decision tree classifier (DecisionTreeClassifier) using PySpark on the assembled_train_data DataFrame.</b>\n",
    "<ul><li>Configuration includes a random seed for reproducibility (seed=1234), specifying the target variable column (labelCol=\"Cover_Type\"), the input feature vector column (featuresCol=\"featureVector\"), and the prediction output column (predictionCol=\"prediction\").</li>\n",
    "    <li>The fit method trains the model, storing it in the model variable.</li>\n",
    "<li>print(model.toDebugString) displays a detailed representation of the trained decision tree model's structure.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",featuresCol=\"featureVector\",predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d505d3",
   "metadata": {},
   "source": [
    "<b>This code snippet converts the feature importances of the trained decision tree model (model) into a Pandas DataFrame using the pd.DataFrame() function from the Pandas library.</b>\n",
    "<ul><li>\n",
    "    The feature importances are extracted from the model using model.featureImportances.toArray().</li>\n",
    "<li>The index of the DataFrame is set to the input column names (input_cols), which represent the features used in the model.</li>\n",
    "    <li>The column containing the feature importances is named \"importance\".</li>\n",
    "<li>The DataFrame is then sorted by the \"importance\" column in descending order using .sort_values(by=\"importance\", ascending=False), allowing users to identify the most influential features in the decision tree model.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72643ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model.featureImportances.toArray(),\n",
    " index=input_cols, columns=['importance']).\\\n",
    " sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b420b",
   "metadata": {},
   "source": [
    "<b>This code applies the trained decision tree model (model) to the assembled_train_data DataFrame to make predictions.</b>\n",
    "<ul><li>\n",
    "The transform method is used to apply the model to the data, resulting in a new DataFrame named predictions.</li><li>\n",
    "The select method is then used to display the \"Cover_Type\" (actual target variable), \"prediction\" (predicted value), and \"probability\" columns from the predictions DataFrame.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29af25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").\\\n",
    "show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0637a5e",
   "metadata": {},
   "source": [
    "<b>This code utilizes PySpark's MulticlassClassificationEvaluator to evaluate the decision tree model's accuracy on the predictions DataFrame.</b>\n",
    "<ul><li>It calculates the accuracy of the model's predictions using setMetricName(\"accuracy\").evaluate(predictions).</li>\n",
    "<li>Additionally, it computes the F1 score, a measure of a model's accuracy, using setMetricName(\"f1\").evaluate(predictions).</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ddbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",predictionCol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94745079",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet creates a confusion matrix from the predictions DataFrame, showing the counts of correct and incorrect predictions for each cover type</li><li>. It groups the predictions by the actual cover types and pivots the data to compare the predicted cover types against the actual ones. The matrix is then sorted by the actual cover types for better interpretation.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").\\\n",
    "pivot(\"prediction\", range(1,8)).count().\\\n",
    "na.fill(0.0).\\orderBy(\"Cover_Type\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2fd2e",
   "metadata": {},
   "source": [
    "<b>This code defines a function class_probabilities that calculates the proportion of each class in a dataset based on the \"Cover_Type\" column. It takes a DataFrame data as input and returns a list of class proportions, sorted by \"Cover_Type\".</b>\n",
    "<ul><li>\n",
    "    The class_probabilities function first counts the total number of rows in the DataFrame. It then groups the data by \"Cover_Type\", calculates the count for each group, and orders the result by \"Cover_Type\".</li>\n",
    "<li>Next, it converts the count to a double type, calculates the proportion of each class count to the total count, and selects the \"count_proportion\" column.</li><li>\n",
    "    Finally, it collects and returns the list of class proportions.</li></ul>\n",
    "<b>The function is applied to both the train_data and test_data DataFrames to calculate the class proportions for the training and test datasets, respectively.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44721dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def class_probabilities(data):\n",
    "    total = data.count()\n",
    "    return data.groupBy(\"Cover_Type\").count().\\\n",
    " orderBy(\"Cover_Type\").\\\n",
    " select(col(\"count\").cast(DoubleType())).\\\n",
    " withColumn(\"count_proportion\", col(\"count\")/total).\\\n",
    " select(\"count_proportion\").collect()\n",
    "\n",
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)\n",
    "train_prior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f955e1d",
   "metadata": {},
   "source": [
    "<b>This code calculates the weighted sum of products of prior probabilities for the training and test datasets.</b>\n",
    "<ul><li>\n",
    "It extracts the class proportions from train_prior_probabilities and test_prior_probabilities using list comprehensions.</li>\n",
    "    <li>The zip function pairs up the class proportions from the training and test datasets.</li><li>\n",
    "The sum function computes the sum of the products of these paired class proportions, reflecting the match between the prior probabilities of the two datasets.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852566c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,test_prior_probabilities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332bfdd",
   "metadata": {},
   "source": [
    "<h3> Tuning Decision Trees</h3><br>\n",
    "<b>\n",
    "This code sets up a PySpark pipeline for assembling features and training a decision tree classifier.</b>\n",
    "<ul><li>\n",
    "It creates a VectorAssembler named assembler to combine input features into a single feature vector.</li><li>\n",
    "    The DecisionTreeClassifier named classifier is configured with specific parameters.</li><li>\n",
    "The Pipeline named pipeline is constructed with stages for the assembler and classifier, enabling a streamlined workflow for feature assembly and model training.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6f19b",
   "metadata": {},
   "source": [
    "<b>This code snippet sets up a parameter grid for tuning the hyperparameters of the decision tree classifier (classifier) using PySpark's ParamGridBuilder.</b>\n",
    "\n",
    "<ul><li>The ParamGridBuilder is used to define a grid of hyperparameters to explore during model training.\n",
    "Hyperparameters such as impurity, maxDepth, maxBins, and minInfoGain are specified with a list of values to try.\n",
    "    The build() method is called to build the parameter grid.</li>\n",
    "<li>Additionally, a MulticlassClassificationEvaluator (multiclassEval) is configured to evaluate the model's performance using accuracy (setMetricName(\"accuracy\")) on the \"Cover_Type\" target variable (setLabelCol(\"Cover_Type\")) and the predicted values column (setPredictionCol(\"prediction\")).</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "addGrid(classifier.maxBins, [40, 300]). \\\n",
    "addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "build()\n",
    "\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "setLabelCol(\"Cover_Type\"). \\\n",
    "setPredictionCol(\"prediction\"). \\\n",
    "setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9492bda",
   "metadata": {},
   "source": [
    "<b>The code sets up a train-validation split for hyperparameter tuning using PySpark's TrainValidationSplit.</b><ul><li>\n",
    "It configures the TrainValidationSplit with parameters such as a random seed (seed=1234), the pipeline (pipeline) containing the assembler and classifier, an evaluator (multiclassEval) for accuracy evaluation, a parameter grid (paramGrid) for hyperparameter tuning, and a training ratio (trainRatio=0.9).</li><li>\n",
    "The fit method is used to train the validator object on the train_data DataFrame, resulting in a validator_model that contains the best model found during the tuning process.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "estimator=pipeline,\n",
    "evaluator=multiclassEval,\n",
    "estimatorParamMaps=paramGrid,\n",
    "trainRatio=0.9)\n",
    "validator_model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667bf9d",
   "metadata": {},
   "source": [
    "<b>This code snippet extracts and prints the best hyperparameters found during the hyperparameter tuning process using PySpark's TrainValidationSplit.</b>\n",
    "<ul><li>\n",
    "    best_model contains the best model selected by the tuning process.</li><li>\n",
    "    best_model.stages[1] accesses the decision tree classifier (classifier) within the pipeline.</li><li>\n",
    "    extractParamMap() retrieves the hyperparameters of the decision tree classifier.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de72778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c12fe",
   "metadata": {},
   "source": [
    "<b>The code fits the validator on the train_data DataFrame to find the best model.</b><ul><li>\n",
    "    It extracts the validation metrics and hyperparameters for each model evaluated during tuning.</li><li>\n",
    "    The metrics and hyperparameters are combined into a list of tuples.</li><li>\n",
    "The list is sorted based on the validation metrics in descending order to identify the best-performing models.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_model = validator.fit(train_data)\n",
    "metrics = validator_model.validationMetrics\n",
    "params = validator_model.getEstimatorParamMaps()\n",
    "metrics_and_params = list(zip(metrics, params))\n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
    "metrics_and_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4281134",
   "metadata": {},
   "source": [
    "<b>This code snippet sorts the validation metrics in descending order to find the best-performing model and evaluates its performance on the test data using the MulticlassClassificationEvaluator.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort(reverse=True)\n",
    "print(metrics[0])\n",
    "multiclassEval.evaluate(best_model.transform(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c7fb7",
   "metadata": {},
   "source": [
    "<h3>Categorical Features Revisited</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a4d10",
   "metadata": {},
   "source": [
    "<b>This code defines a function called unencode_one_hot that reverses the one-hot encoding of \"Wilderness_Area\" and \"Soil_Type\" columns in a DataFrame.</b><ul><li> It first creates a VectorAssembler for each set of one-hot encoded columns, combining them into single columns named \"wilderness\" and \"soil\".</li><li> Then, it defines a user-defined function (UDF) to convert these combined columns back to their original categorical values by finding the index of the non-zero element.</li><li> The function applies this UDF to each \"Wilderness_Area\" and \"Soil_Type\" column, converting them back to integer values.</li><li> Finally, it drops the original one-hot encoded columns and renames the newly created columns to \"wilderness\" and \"soil\".</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def unencode_one_hot(data):\n",
    "    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "    wilderness_assembler = VectorAssembler().\\\n",
    " setInputCols(wilderness_cols).\\\n",
    " setOutputCol(\"wilderness\")\n",
    "    unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n",
    "    with_wilderness = wilderness_assembler.transform(data).\\\n",
    " drop(*wilderness_cols).\\\n",
    " withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n",
    "    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "    soil_assembler = VectorAssembler().\\\n",
    " setInputCols(soil_cols).\\\n",
    " setOutputCol(\"soil\")\n",
    "    with_soil = soil_assembler.\\\n",
    " transform(with_wilderness).\\\n",
    " drop(*soil_cols).\\\n",
    " withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n",
    "    return with_soil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9752f0",
   "metadata": {},
   "source": [
    "<p>This code applies the unencode_one_hot function to the train_data DataFrame, reversing the one-hot encoding of \"Wilderness_Area\" and \"Soil_Type\" columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb6521",
   "metadata": {},
   "source": [
    "This code groups the unenc_train_data DataFrame by the \"wilderness\" column and counts the occurrences of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data.groupBy('wilderness').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6277287",
   "metadata": {},
   "source": [
    "<b>This code sets up a PySpark pipeline for a decision tree classifier with vector indexing.</b>\n",
    "<ul><li>\n",
    "    A VectorAssembler combines input columns into a single feature vector column.</li><li>\n",
    "A VectorIndexer indexes categorical features in the feature vector column with a maximum of 40 categories.</li><li>\n",
    "A decision tree classifier is configured with the target variable column and input and output column names for features and predictions.</li><li>\n",
    "The pipeline is constructed with stages for the assembler, indexer, and classifier, facilitating a streamlined workflow </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().\\\n",
    " setMaxCategories(40).\\\n",
    " setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").\\\n",
    " setFeaturesCol(\"indexedVector\").\\\n",
    " setPredictionCol(\"prediction\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a710e",
   "metadata": {},
   "source": [
    "<h3>Random forest takes too long to run</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61475c",
   "metadata": {},
   "source": [
    "<b>This code snippet defines a random forest classifier in PySpark with specific parameters.</b>\n",
    "<ul><li>\n",
    "    RandomForestClassifier is used to define a random forest classifier model.</li><li>\n",
    "Parameters include a random seed for reproducibility (seed=1234), the target variable column (\"Cover_Type\"), the input feature vector column (\"indexedVector\"), and the output prediction column (\"prediction\").</li><li>\n",
    "The columns attribute of unenc_train_data is accessed to display the column names in the DataFrame.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af492146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"indexedVector\",\n",
    "predictionCol=\"prediction\")\n",
    "unenc_train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7b87a",
   "metadata": {},
   "source": [
    "<b>This code snippet sets up a PySpark pipeline for a random forest classifier with hyperparameter tuning.</b><ul><li> It starts by defining the input columns for feature assembly, excluding \"Cover_Type\".</li><li> The VectorAssembler combines these columns into a single feature vector.</li><li> Next, a VectorIndexer is used to index categorical features in the vector.</li><li> The pipeline is constructed with stages for the assembler, indexer, and classifier. </li><li>Hyperparameter tuning is performed using TrainValidationSplit with a specified parameter grid. </li><li>Finally, the best model is determined using the fit method on the validator object with the training data.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().\\\n",
    " setMaxCategories(40).\\\n",
    " setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    " addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    " addGrid(classifier.maxDepth, [1, 20]). \\\n",
    " addGrid(classifier.maxBins, [40, 300]). \\\n",
    " addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    " build()\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    " setLabelCol(\"Cover_Type\"). \\\n",
    " setPredictionCol(\"prediction\"). \\\n",
    " setMetricName(\"accuracy\")\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    " estimator=pipeline,\n",
    " evaluator=multiclassEval,\n",
    "estimatorParamMaps=paramGrid,\n",
    " trainRatio=0.9)\n",
    "validator_model = validator.fit(unenc_train_data)\n",
    "best_model = validator_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b431687",
   "metadata": {},
   "source": [
    "<b>This code extracts and prints the feature importances from the best random forest model found during hyperparameter tuning.</b><ul><li>\n",
    "\n",
    "forest_model = best_model.stages[2] accesses the random forest model (classifier) within the pipeline.</li><li>\n",
    "feature_importance_list is created as a list of tuples, where each tuple contains a feature column name (input_cols) and its corresponding importance score from the random forest model.</li><li>\n",
    "    The list is sorted based on the importance scores in descending order.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = best_model.stages[2]\n",
    "feature_importance_list = list(zip(input_cols,forest_model.featureImportances.toArray()))\n",
    "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pprint(feature_importance_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780d514",
   "metadata": {},
   "source": [
    "<h3>Making Predictions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b563da3",
   "metadata": {},
   "source": [
    "This code prepares the test data by reversing the one-hot encoding of certain columns. Then, it applies the best model found during training to predict the \"Cover_Type\" values for the test data. The prediction for the first row is displayed to provide a glimpse of the model's performance on unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56831ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_test_data = unencode_one_hot(test_data)\n",
    "best_model.transform(unenc_test_data.drop(\"Cover_Type\")).\\select(\"prediction\").show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
