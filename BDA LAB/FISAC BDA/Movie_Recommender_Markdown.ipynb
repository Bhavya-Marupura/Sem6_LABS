{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256a9ef5",
   "metadata": {},
   "source": [
    "<h2>Movie Recommender</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec75e8a",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This code snippet sets up a PySpark environment in a Python script.</li><li> It first imports the necessary modules like pyspark, os, and sys.</li><li> Then, it sets the Python executable for PySpark to the same one being used by the script.</li><li> Finally, it imports the SparkContext class for creating RDDs and the SparkSession class for programming Spark with the DataFrame API.</li>\n",
    "    <li>Then, it attempts to create a Spark session (a SparkContext in older versions of Spark) with the configuration to use all available CPU cores on the local machine.</li><li> If the session is successfully created, it prints a message. If a SparkContext already exists, it raises a warning.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "try:\n",
    "    sc = ps.SparkContext('local[*]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd118690",
   "metadata": {},
   "source": [
    "<ul><li>\n",
    "This code defines a unit test for a PySpark RDD operation using the unittest framework. It includes a test case TestRdd with a method test_take that checks if the take(4) method on an RDD created from [1, 2, 3, 4] returns the same list [1, 2, 3, 4].</li><li> The run_tests function loads this test case into a test suite and runs it, displaying the results with a verbosity level of 1.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4339c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import sys\n",
    "class TestRdd(unittest.TestCase):\n",
    "    def test_take(self):\n",
    "        input = sc.parallelize([1,2,3,4])\n",
    "        self.assertEqual([1,2,3,4], input.take(4))\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase( TestRdd )\n",
    "    unittest.TextTestRunner(verbosity=1,stream=sys.stderr).run( suite )\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df3dc5",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet reads a JSON file containing movie reviews into a PySpark RDD. It defines different sets of fields that each review should have.</li><li> It then filters the RDD to keep only the reviews that contain all the fields specified in one of the field sets (fields2). The filtered RDD is then cached for faster access.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "fields = ['product_id',\n",
    " 'user_id',\n",
    " 'score',\n",
    " 'time']\n",
    "fields2 = ['product_id',\n",
    " 'user_id',\n",
    " 'review',\n",
    "'profile_name',\n",
    " 'helpfulness',\n",
    " 'score',\n",
    " 'time']\n",
    "fields3 = ['product_id',\n",
    " 'user_id',\n",
    " 'time']\n",
    "fields4 = ['user_id',\n",
    " 'score',\n",
    " 'time']\n",
    " \n",
    "def validate(line):\n",
    "    for field in fields2:\n",
    "        if field not in line: return False\n",
    "    return True\n",
    "reviews_raw = sc.textFile('data/movies.json')\n",
    "reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n",
    "reviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733456e1",
   "metadata": {},
   "source": [
    "<b>This code calculates the number of unique movies, unique users, and total entries in the reviews RDD.</b><ul><li> It first groups the reviews by 'product_id' (movie ID) and 'user_id' (user ID) using groupBy and then counts the number of unique IDs for each group using count().</li><li> Finally, it prints the total number of reviews, unique movies, and unique users in the dataset.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b96f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n",
    "num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n",
    "num_entries = reviews.count()\n",
    " \n",
    "print (str(num_entries) + \" reviews of \" + str(num_movies) + \" movies by \" +str(num_users) + \" different people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b8d49",
   "metadata": {},
   "source": [
    "<b>This code snippet calculates the average score for each product in the reviews RDD:</b>\n",
    "<ul><li>\n",
    "It first maps each review to a tuple ((product_id,), 1), where product_id is the key and 1 is the value.</li><li>\n",
    "It then uses mapValues to convert the values to (value, 1) tuples, where the first element is the score and the second element is 1.</li><li>\n",
    "    It uses reduceByKey to sum up the scores and counts for each product.</li><li>\n",
    "    It filters out products that have less than 20 reviews.</li><li>\n",
    "It maps the average scores to (score_sum, review_count) tuples and reorders them based on the sum of scores and review counts in descending order.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n",
    "avg3 = r1.mapValues(lambda x: (x, 1)) \\\n",
    " .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \n",
    "avg3 = avg3.filter(lambda x: x[1][1] > 20 )\n",
    "avg3 = avg3.map(lambda x: ((x[1][0]+x[1][1],), x[0])) \\\n",
    " .sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577b143",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet iterates over the first 10 elements of the avg3 RDD (assuming avg3 is an RDD containing tuples of the form ((score_sum, review_count), product_id)). It then prints a formatted string for each movie, including a URL to Amazon's product page for that movie, the movie's ID, and the number of reviews for the movie.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg3.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \"str(movie[0][0]) + \" PEOPLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f24c1",
   "metadata": {},
   "source": [
    "<b>This code calculates the average score given by each user in a set of movie reviews.</b><ul><li> It first maps each review to a tuple containing the user ID and a value of 1.</li><li> Then, it calculates the total score and the count of reviews for each user using reduceByKey.</li><li> It filters out users with less than 20 reviews and calculates the average score for each user.</li><li> Finally, it sorts the results based on the average score in descending order.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64264b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n",
    "avg2= r2.mapValues(lambda x: (x, 1)) \\\n",
    " .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "avg2 = avg2.filter(lambda x: x[1][1] > 20 )\n",
    "avg2 = avg2.map(lambda x: ((x[1][0]+x[1][1],), x[0])) \\\n",
    " .sortByKey(ascending=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b37ffb",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet iterates over the first 10 elements of the avg2 RDD (assuming avg2 is an RDD containing tuples of the form ((score_sum, review_count), user_id)). It then prints a formatted string for each user, including a URL to Amazon's product page for that user's reviews, the user's ID, and the number of reviews by the user.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef749e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg2.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \"str(movie[0][0]) + \" MOVIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869d338",
   "metadata": {},
   "source": [
    "<b>This code filters the reviews RDD to keep only the entries where the 'profile_name' contains the string \"George\".</b><ul><li> It then counts the number of such entries and prints the count.</li><li> Next, it collects and iterates over all the filtered reviews, printing various details for each review, such as rating, helpfulness, product ID (with an Amazon URL), summary, and full review text.</li></ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n",
    "print (\"Found \" + str(filtered.count()) + \" entries.\\n\")\n",
    "\n",
    "for review in filtered.collect():\n",
    "    print (\"Rating: \" + str(review['score']) + \" and helpfulness: \" + review['helpfulness'])\n",
    "    print (\"http://www.amazon.com/dp/\" + review['product_id'])\n",
    "    print (review['summary'])\n",
    "    print (review['review'])\n",
    "    print (\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7caade",
   "metadata": {},
   "source": [
    "<b>This code calculates the average score for each movie in a set of reviews.</b><ul><li> It first maps each review to a tuple containing the movie ID and the score.</li><li> Then, it calculates the total score and the count of reviews for each movie using reduceByKey.</li><li> It filters out movies with less than 20 reviews and calculates the average score for each movie.</li><li> Finally, it sorts the results based on the average score in ascending order.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeeb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))\n",
    "avg = reviews_by_movie.mapValues(lambda x: (x, 1)) \\\n",
    " .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "avg = avg.filter(lambda x: x[1][1] > 20 )\n",
    "avg = avg.map(lambda x: ((x[1][0]/x[1][1],), x[0])) \\\n",
    " .sortByKey(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd9557",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet iterates over the first 10 elements of the avg RDD (assuming avg is an RDD containing tuples of the form ((average_score,), movie_id)). It then prints a formatted string for each movie, including a URL to Amazon's product page for that movie, the movie's ID, and the average rating.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ee0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg.take(10):\n",
    "    print (\"http://www.amazon.com/dp/\" + movie[1][0] + \"RatingL \"+str(movie[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c776145",
   "metadata": {},
   "source": [
    "<h3>Spark and Pandas</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fa2a0",
   "metadata": {},
   "source": [
    "<ul><li>This code creates a new RDD called timeseries_rdd by mapping each entry in the reviews RDD to a dictionary containing the score and the timestamp converted to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'],'time': datetime.fromtimestamp(entry['time'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0a450",
   "metadata": {},
   "source": [
    "<b>This code snippet uses the pandas library to create a time series analysis of the sampled timeseries_rdd RDD:<b>\n",
    "<ul><li>\n",
    "    It defines a parser function to convert the timestamp to a datetime object.</li><li>\n",
    "It samples the timeseries_rdd RDD to reduce the number of entries for plotting purposes.</li><li>\n",
    "It creates a Pandas DataFrame from the sampled RDD, specifying the columns as 'score' and 'time'.</li><li>\n",
    "It prints the first 3 rows of the DataFrame.</li><li></li><li>\n",
    "It sets the index of the DataFrame to the 'time' column.</li><li>\n",
    "It resamples the 'score' column of the DataFrame to get the count of scores for each year, month, and quarter.</li><li>\n",
    "    It plots the resampled data for each frequency (yearly, monthly, and quarterly).</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "sample = timeseries_rdd.sample(withReplacement=False, fraction=20000.0/num_entries, seed=1134)\n",
    "timeseries = pd.DataFrame(sample.collect(),\n",
    "columns=['score', 'time'])\n",
    "print(timeseries.head(3))\n",
    "timeseries.score.astype('float64')\n",
    "\n",
    "timeseries.set_index('time', inplace=True)\n",
    "Rsample = timeseries.score.resample('Y').count()\n",
    "Rsample.plot()\n",
    "Rsample2 = timeseries.score.resample('M').count()\n",
    "Rsample2.plot()\n",
    "Rsample3 = timeseries.score.resample('Q').count()\n",
    "Rsample3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91dc63d",
   "metadata": {},
   "source": [
    "<h3>Matrix Factorization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92419ef3",
   "metadata": {},
   "source": [
    "<b>This code snippet creates a histogram showing the average rating for each of the top 4 movies.</b><ul><li> It iterates over the first 4 elements of the avg RDD (assuming avg is an RDD containing tuples of the form ((average_score,), movie_id)).</li><li> For each movie, it uses plt.bar to create a bar chart where the x-axis represents the movie ID and the y-axis represents the average rating.</li><li> The title of the plot is set to 'Histogram of 'AVERAGE RATING OF MOVIE'', the x-axis label is 'MOVIE', and the y-axis label is 'AVGRATING'.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e983f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg.take(4):\n",
    "    plt.bar(movie[1][0],movie[0][0])\n",
    "    plt.title('Histogram of \\'AVERAGE RATING OF MOVIE\\'')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('AVGRATING')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b6ece",
   "metadata": {},
   "source": [
    "<b>This code generates a histogram displaying the number of movies reviewed by the top 3 users. </b><ul><li>It loops through the first 3 elements of the avg2 RDD, where each element represents a user and their respective movie count.</li><li> For each user, a bar is plotted with the user ID on the x-axis and the movie count on the y-axis. </li><li>The plot is titled 'Histogram of 'NUMBER OF MOVIES REVIEWED BY USER'', with 'USER' and 'MOVIE COUNT' labels for the x-axis and y-axis, respectively.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg2.take(3):\n",
    "    plt.bar(movie[1][0],movie[0][0])\n",
    "    plt.title('Histogram of \\'NUMBER OF MOVIES REVIEWED BY USER\\'')\n",
    "    plt.xlabel('USER')\n",
    "    plt.ylabel('MOVIE COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dfe8d",
   "metadata": {},
   "source": [
    "<b>This code generates a histogram showing the number of users who have reviewed each of the top 4 movies.</b><ul><li> It loops through the first 4 elements of the avg3 RDD, where each element represents a movie and the number of users who have reviewed it.</li><li> For each movie, a bar is plotted with the movie ID on the x-axis and the user count on the y-axis. </li><li>The plot is titled 'Histogram of 'MOVIES REVIEWED BY NUMBER OF USERS'', with 'MOVIE' and 'USER COUNT' labels for the x-axis and y-axis, respectively.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg3.take(4):\n",
    "    plt.bar(movie[1][0],movie[0][0])\n",
    "    plt.title('Histogram of \\'MOVIES REVIEWED BY NUMBER OF USERS\\'')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('USER COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d868e",
   "metadata": {},
   "source": [
    "<b>This code snippet prepares data for collaborative filtering using the Alternating Least Squares (ALS) algorithm from PySpark's pyspark.mllib.recommendation module:</b>\n",
    "<ul><li>\n",
    "    It defines a function get_hash to hash user and product IDs into integers for anonymity.</li><li>\n",
    "It maps each review in reviews RDD to a tuple of hashed user ID, hashed product ID, and the rating.</li><li>\n",
    "    It filters the data into training and testing sets based on a hashing condition.</li><li>\n",
    "    It caches the training data for faster access.</li><li>\n",
    "    It prints the number of samples in the training and testing datasets.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from numpy import array\n",
    "import hashlib\n",
    "import math\n",
    "def get_hash(s):\n",
    "    return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "ratings = reviews.map(lambda entry: tuple([ get_hash(entry['user_id'].encode('utf-8')),get_hash(entry['product_id'].encode('utf-8')),int(entry['score']) ]))\n",
    "\n",
    "train_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) >=2 )\n",
    "test_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) < 2 )\n",
    "train_data.cache()\n",
    "\n",
    "print (\"Number of train samples: \" + str(train_data.count()))\n",
    "print (\"Number of test samples: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9295d4",
   "metadata": {},
   "source": [
    "<b>This code evaluates a recommendation model by predicting ratings for test data and calculating the Mean Squared Error (MSE) between predicted and actual ratings. </b><ul><li>It first trains an ALS model using train_data, specifying the rank and number of iterations.</li><li> Then, it maps the test data to user-product pairs and uses the model to predict ratings.</li><li> These predictions are joined with the actual ratings, and MSE is calculated by comparing predicted and actual ratings for each pair.</li><li> The code returns the first 10 pairs of actual and predicted ratings for examination.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "rank = 20\n",
    "numIterations = 20\n",
    "model = ALS.train(train_data, rank, numIterations)\n",
    "def convertToFloat(lines):\n",
    "    returnedLine = []\n",
    "    for x in lines:\n",
    "        returnedLine.append(float(x))\n",
    "    return returnedLine\n",
    " # Evaluate the model on test data\n",
    "unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))\n",
    "predictions= model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))\n",
    "true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)\n",
    "MSE = true_and_predictions.map(lambda r: (int(r[1][0])- int(r[1][1])**2).reduce(lambda x, y: x + y)/true_and_predictions.count())\n",
    "true_and_predictions.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb1b6a",
   "metadata": {},
   "source": [
    "<h3>No demo without word count example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11d75e",
   "metadata": {},
   "source": [
    "<b>This code snippet analyzes reviews to identify frequently occurring words in both positive (score 5.0) and negative (score 1.0) reviews:</b>\n",
    "<ul><li>\n",
    "    It filters the reviews to separate positive and negative reviews based on their scores.</li><li>\n",
    "It extracts individual words from the reviews and counts the occurrences of each word.</li><li>\n",
    "It filters out words that occur less than min_occurrences times in either positive or negative reviews.</li><li>\n",
    "The result is a list of frequently occurring words in positive and negative reviews, along with their respective counts.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurrences = 10\n",
    "good_reviews = reviews.filter(lambda line: line['score']==5.0)\n",
    "bad_reviews = reviews.filter(lambda line: line['score']==1.0)\n",
    "\n",
    "good_words = good_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_good_words = good_words.count()\n",
    "good_words = good_words.map(lambda word: (word.strip(), 1)) \\\n",
    " .reduceByKey(lambda a, b: a+b) \\\n",
    " .filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_bad_words = bad_words.count()\n",
    "bad_words = bad_words.map(lambda word: (word.strip(), 1)) \\\n",
    " .reduceByKey(lambda a, b: a+b) \\\n",
    " .filter(lambda word_count: word_count[1] > min_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7495583",
   "metadata": {},
   "source": [
    "<b>This code snippet calculates the normalized frequency of words in positive (good) and negative (bad) reviews.</b><ul><li> It iterates over the words in each set of reviews and divides the count of each word by the total number of words in that set.</li><li> This normalization allows comparison of word frequencies between the two sets.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_good = good_words.map(lambda word: ((word[0],), float(word[1])/num_good_words))\n",
    "frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1])/num_bad_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a5d78",
   "metadata": {},
   "source": [
    "<ul><li>This code joins the normalized word frequencies for good and bad reviews based on the words.</li><li> It creates a new RDD joined_frequencies by combining the frequencies of words that appear in both good and bad reviews.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " joined_frequencies = frequency_good.join(frequency_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8d24e",
   "metadata": {},
   "source": [
    "<b>This code calculates the relative difference in word frequencies between positive (good) and negative (bad) reviews and sorts them in descending order.</b><ul><li> It first defines a function to compute the relative difference between two values. </li><li>Then, it maps each word's frequency to a tuple containing the relative difference and the word. </li><li>Finally, it sorts the results based on the relative difference, showing which words are more characteristic of one type of review compared to the other.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def relative_difference(a, b):\n",
    "    return math.fabs(a-b)/a\n",
    "result = joined_frequencies.map(lambda f: ((relative_difference(f[1][0], f[1][1]),), f[0][0]) ) \\\n",
    " .sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018d5bc",
   "metadata": {},
   "source": [
    "<b>This code creates a histogram displaying the relative differences in word frequencies between positive (good) and negative (bad) reviews for the top 7 words.</b><ul><li> It loops through the first 7 elements of the result RDD, where each element represents a word and its relative difference in frequency.</li><li> For each word, a bar is plotted with the word on the x-axis and the relative difference on the y-axis.</li><li> The plot is titled 'Histogram of 'SENTIMENT ANALYSIS'', with 'WORD' and 'NUMBER OF OCCURRENCES' labels for the x-axis and y-axis, respectively.</li></ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b004f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in result.take(7):\n",
    "    plt.bar(movie[1],movie[0][0])\n",
    "    plt.title('Histogram of \\'SENTIMENT ANALYSIS\\'')\n",
    "    plt.xlabel('WORD')\n",
    "    plt.ylabel('NUMBER OF OCCURANCES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7151fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
