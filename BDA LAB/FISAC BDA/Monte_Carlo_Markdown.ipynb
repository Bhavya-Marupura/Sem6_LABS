{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc62b45",
   "metadata": {},
   "source": [
    "<h2>Monte Carlo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23438df9",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This code snippet sets up a PySpark environment in a Python script.</li><li> It first imports the necessary modules like pyspark, os, and sys.</li><li> Then, it sets the Python executable for PySpark to the same one being used by the script.</li><li> Finally, it imports the SparkContext class for creating RDDs and the SparkSession class for programming Spark with the DataFrame API.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274eb88",
   "metadata": {},
   "source": [
    "<ul><li>This code creates a SparkSession named spark with specific configuration options. It sets the driver memory to 16 GB and names the application 'chapter_8.</li><ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_8').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af9439",
   "metadata": {},
   "source": [
    "<h3>Preparing the Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb682e6f",
   "metadata": {},
   "source": [
    "<ul><li>This code reads three CSV files (ABAX.csv, AAME.csv, and AEPI.csv) located in the data/stocksA/ directory into a Spark DataFrame named stocks.</li><li> The header='true' argument indicates that the first row of each CSV file contains the column names, and inferSchema='true' tells Spark to infer the data types of each column.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
    "stocks.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d444d7",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet adds a new column \"Symbol\" to the DataFrame stocks.</li><li> It extracts the file name from the full file path and assigns it to the \"Symbol\" column.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24466d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "stocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).\\withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"),-1)).\\withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n",
    "stocks.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322c4a5",
   "metadata": {},
   "source": [
    "<b>This code snippet reads multiple CSV files into a DataFrame factors and adds a new column \"Symbol\" to it.</b><ul><li> The first withColumn call adds the full file path of each row as a new column called \"Symbol\".</li><li> The second withColumn call extracts the file name from the full path by splitting the path using \"/\" as the delimiter and taking the last element.</li><li> Finally, the third withColumn call further refines the \"Symbol\" column by splitting the file name using \".\" as the delimiter and taking the first element.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
    "factors = factors.withColumn(\"Symbol\", fun.input_file_name()).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"),-1)). withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc7068",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet adds a new column \"count\" to the stocks DataFrame using the Window function to count the occurrences of each symbol.</li><li> It then filters the DataFrame to keep only rows where the count is greater than 260 multiplied by 5 plus 10.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "stocks = stocks.withColumn('count', fun.count('Symbol').over(Window.partitionBy('Symbol'))).filter(fun.col('count') > 260*5 + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590f690",
   "metadata": {},
   "source": [
    "<ul><li>This code sets a Spark SQL configuration spark.sql.legacy.timeParserPolicy to LEGACY, which specifies the behavior of the time parser for legacy datetime functions in Spark SQL. </li><li>This configuration affects how timestamps and dates are parsed in SQL queries.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92d84b",
   "metadata": {},
   "source": [
    "<ul><li>This code converts the 'Date' column in the stocks DataFrame to a standard date format. It first converts the 'Date' column to a timestamp using the 'dd-MMM-yy' format, and then converts this timestamp to a date format. </li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n",
    "stocks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d923f",
   "metadata": {},
   "source": [
    "<ul><li>This code filters the stocks DataFrame to include only rows where the 'Date' column falls within the range from October 23, 2009, to October 23, 2014.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad698805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "stocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\filter(fun.col('Date') <= datetime(2014, 10, 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e072d",
   "metadata": {},
   "source": [
    "<ul><li>This code converts the 'Date' column in the factors DataFrame to a date format and then filters the DataFrame to include only rows where the 'Date' falls within the range from October 23, 2009, to October 23, 2014, similar to what was done with the stocks DataFrame.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = factors.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n",
    "factors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\filter(fun.col('Date') <= datetime(2014, 10, 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcf628",
   "metadata": {},
   "source": [
    "<ul><li>This code converts the Spark DataFrames stocks and factors into Pandas DataFrames stocks_pd_df and factors_pd_df, respectively, and then displays the first 5 rows of the factors_pd_df DataFrame.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2771a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_pd_df = stocks.toPandas()\n",
    "factors_pd_df = factors.toPandas()\n",
    "factors_pd_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25460989",
   "metadata": {},
   "source": [
    "<h3>Determining Factor Weights</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7536e",
   "metadata": {},
   "source": [
    "<ul><li>This code calculates rolling returns for both stock and factor dataframes using a custom function my_fun, which calculates the returns based on the close price. It then resets the index and sorts the dataframes by the rolling window index level_1.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "def my_fun(x):\n",
    "    return ((x.iloc[-1]- x.iloc[0]) / x.iloc[0])\n",
    "stock_returns = stocks_pd_df.groupby('Symbol').Close.\\\n",
    " rolling(window=n_steps).apply(my_fun)\n",
    "factors_returns = factors_pd_df.groupby('Symbol').Close.\\\n",
    " rolling(window=n_steps).apply(my_fun)\n",
    "stock_returns = stock_returns.reset_index().\\\n",
    " sort_values('level_1').\\\n",
    " reset_index()\n",
    "factors_returns = factors_returns.reset_index().\\\n",
    " sort_values('level_1').\\\n",
    " reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62def820",
   "metadata": {},
   "source": [
    "<b>This code combines the stock and factor dataframes with their respective rolling returns.</b><ul><li> It assigns the rolling returns as new columns in the dataframes. For the factors dataframe, it also calculates the squared rolling returns. </li><li>The dataframes are then pivoted to have symbols as columns and dates as rows. Finally, the columns are renamed to have a clear structure.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80df16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined stocks DF\n",
    "stocks_pd_df_with_returns = stocks_pd_df.assign(stock_returns = stock_returns['Close'])\n",
    "# Create combined factors DF\n",
    "factors_pd_df_with_returns = factors_pd_df.assign(factors_returns = factors_returns['Close'],factors_returns_squared = factors_returns['Close']**2)\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.pivot(index='Date',columns='Symbol',values=['factors_returns', 'factors_returns_squared'])\n",
    "factors_pd_df_with_returns.columns = factors_pd_df_with_returns.columns.to_series().str.join('_').reset_index()[0]\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.reset_index()\n",
    "\n",
    "print(factors_pd_df_with_returns.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88835db8",
   "metadata": {},
   "source": [
    "<ul><li>The code prints the columns of the factors_pd_df_with_returns DataFrame.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(factors_pd_df_with_returns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde7a51",
   "metadata": {},
   "source": [
    "<b>The code imports necessary libraries and defines a function to perform linear regression for each stock based on the provided features.</b><ul><li> It then merges the stock and factor DataFrames, drops any rows with missing values in the feature columns or 'stock_returns', and calculates the coefficients for each stock using linear regression.</li><li> The final output is a DataFrame (coefs_per_stock) containing the symbol and coefficients for each feature column.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc402626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    " # For each stock, create input DF for linear regression training\n",
    "stocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,\n",
    " factors_pd_df_with_returns,how=\"left\", on=\"Date\")\n",
    "feature_columns = list(stocks_factors_combined_df.columns[-6:])\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    stocks_factors_combined_df = stocks_factors_combined_df.dropna(subset=feature_columns  + ['stock_returns'])\n",
    "def find_ols_coef(df):\n",
    "    y = df[['stock_returns']].values\n",
    "    X = df[feature_columns]\n",
    "    regr = LinearRegression()\n",
    "    regr_output = regr.fit(X, y)\n",
    "    return list(df[['Symbol']].values[0]) + list(regr_output.coef_[0])\n",
    "\n",
    "coefs_per_stock = stocks_factors_combined_df.groupby('Symbol').apply(find_ols_coef)\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\n",
    "coefs_per_stock.columns = ['symbol', 'factor_coef_list']\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock.factor_coef_list.tolist(),index=coefs_per_stock.index,columns = ['Symbol'] + feature_columns)\n",
    "coefs_per_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78a30e",
   "metadata": {},
   "source": [
    "<h2>Sampling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ba9b6",
   "metadata": {},
   "source": [
    "<ul><li>This code snippet selects the 'Close' values from the 'factors_returns' DataFrame for a specific symbol (the first symbol in the DataFrame) and plots the kernel density estimate (KDE) of these values.</li><li> The KDE plot gives an estimate of the probability density function of the variable.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n",
    "samples.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424e747",
   "metadata": {},
   "source": [
    "<b>This code calculates the correlation matrix for the 'Close' values of three different symbols ('f_1', 'f_2', and 'f_3') in the 'factors_returns' DataFrame.</b><ul><li> It selects the 'Close' values for each symbol and creates a DataFrame with these values.</li><li> The correlation matrix shows how each pair of factors is correlated with each other.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n",
    "f_2 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[1]]['Close']\n",
    "f_3 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[2]]['Close']\n",
    "\n",
    "print(f_1.size,len(f_2),f_3.size)\n",
    "pd.DataFrame({'f1': list(f_1)[1:1040], 'f2': list(f_2)[1:1040], 'f3': list(f_3)}).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc48a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_returns_cov = pd.DataFrame({'f1': list(f_1)[1:1040],\n",
    " 'f2': list(f_2)[1:1040],\n",
    " 'f3': list(f_3)})\\\n",
    " .cov().to_numpy()\n",
    "factors_returns_mean = pd.DataFrame({'f1': list(f_1)[1:1040],\n",
    " 'f2': list(f_2)[1:1040],\n",
    " 'f3': list(f_3)}).\\\n",
    " mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda4ef3",
   "metadata": {},
   "source": [
    "<ul><li>This line of code generates a random sample from a multivariate normal distribution with the mean vector factors_returns_mean and covariance matrix factors_returns_cov.</li><li> The result is a set of values that simulate a random observation of the factors.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "multivariate_normal(factors_returns_mean, factors_returns_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebec21",
   "metadata": {},
   "source": [
    "<h3>Running the Trails</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\n",
    "b_feature_columns = spark.sparkContext.broadcast(feature_columns)\n",
    "b_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\n",
    "b_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd10f3",
   "metadata": {},
   "source": [
    "<ul><li>The code creates a list of seeds for random number generation (seeds) and converts it into a Spark DataFrame (seedsDF) with an IntegerType schema.</li><li> It then repartitions the DataFrame into parallelism partitions. </li><li>This partitioning can help distribute the seed values evenly across the Spark cluster, potentially improving parallelism and performance for tasks that use these seeds.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "parallelism = 1000\n",
    "num_trials = 1000000\n",
    "base_seed = 1496\n",
    "seeds = [b for b in range(base_seed,base_seed + parallelism)]\n",
    "seedsDF = spark.createDataFrame(seeds, IntegerType())\n",
    "seedsDF = seedsDF.repartition(parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2617c",
   "metadata": {},
   "source": [
    "<b>This code defines a function calculate_trial_return that is intended to calculate trial returns based on some random factors and coefficients per stock.</b>\n",
    "<ul><li> It then iterates through a specified number of trials, each time setting a random seed and generating random factors.</li><li> It calculates the returns for each stock based on these factors and coefficients, and appends the total return for all stocks to a list.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee31a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import seed\n",
    "from pyspark.sql.types import LongType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "def calculate_trial_return(x):\n",
    "    trial_return_list = []\n",
    "    for i in range(int(num_trials/parallelism)):\n",
    "        random_int = random.randint(0, num_trials*num_trials)\n",
    "        seed(x)\n",
    "        random_factors = multivariate_normal(b_factors_returns_mean.value,b_factors_returns_cov.value)\n",
    "\n",
    "        coefs_per_stock_df = b_coefs_per_stock.value\n",
    "        returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] *(list(random_factors) + list(random_factors**2)))\n",
    "        trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()/b_coefs_per_stock.value.size))\n",
    "    return trial_return_list\n",
    "\n",
    "udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98022ec2",
   "metadata": {},
   "source": [
    "<b>This code generates random trials to simulate stock returns. </b><ul><li>It creates a DataFrame seedsDF with random seeds, then uses these seeds to generate random factors and calculate returns for each stock.</li><li> The calculate_trial_return function generates random factors, multiplies them by the coefficients per stock, and calculates the sum of returns for each trial.</li><li> This is done for multiple trials, and the results are stored in the trials DataFrame.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "trials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\n",
    "trials = trials.select('value', explode('trial_return').alias('trial_return'))\n",
    "trials.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094d768",
   "metadata": {},
   "source": [
    "<h3>Takes Some Time</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7474d",
   "metadata": {},
   "source": [
    "<b>This code finds the 5th percentile of the 'trial_return' values in the trials DataFrame.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20881f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.approxQuantile('trial_return', [0.05], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce80bf8",
   "metadata": {},
   "source": [
    "<b>This code calculates the average of the 5% lowest trial_return values in the trials DataFrame.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.orderBy(col('trial_return').asc()).limit(int(trials.count()/20)).agg(fun.avg(col(\"trial_return\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ca8b9",
   "metadata": {},
   "source": [
    "<h3>Visualizing the Distribution of Returns</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0258578",
   "metadata": {},
   "source": [
    "<b>\n",
    "This code converts the Spark DataFrame trials to a Pandas DataFrame mytrials and then plots a line graph of the data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41820c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "mytrials=trials.toPandas()\n",
    "mytrials.plot.line()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
